[
  {
    "objectID": "Projects/project_gallery.html",
    "href": "Projects/project_gallery.html",
    "title": "Project Gallery",
    "section": "",
    "text": "Goalkeepers and their heights\n\n\n\n\n\n\nCameron West and Stéphane Guillou\n\n\nApr 16, 2025\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Workshops",
      "The Project",
      "Project Gallery"
    ]
  },
  {
    "objectID": "Projects/details.html",
    "href": "Projects/details.html",
    "title": "Overview",
    "section": "",
    "text": "The best way to learn is by doing. That’s why, over the these three days, you are tasked with analysing, visualising and reporting on a set of data!\nRoughly 50% of our intensive is dedicated to working on the project. Working in groups of 2-4, you’ll need to use the techniques we learn to draw some observations about your chosen dataset.\nThe end goal will be a quick fire (low stakes) one-minute presentation with a dashboard to complement.\nSee the datasets page for the data and below for submission details.",
    "crumbs": [
      "Workshops",
      "The Project",
      "Overview"
    ]
  },
  {
    "objectID": "Projects/details.html#project-outline",
    "href": "Projects/details.html#project-outline",
    "title": "Overview",
    "section": "Project outline",
    "text": "Project outline\nThere are a few key requirements for the project, but otherwise it’s up to you!\n\nAnalyse a dataset and create some visualisations\nPut together a final dashboard-style report. Anything from a powerpoint slide to an interactive dashboard will do!\nDeliver a quick fire one-minute presentation with your group\n\nWe’ll have 5 sessions during the intensive days to work on the project, just shy of six hours. The goal of these sessions is twofold\n\nTo work on the project, analysing data and creating visualisations\nTo dive deeper into the content, perfect for questions and conversations\n\nWhile you’ll be working in groups, everyone should practise analysing and visualising the data. We recommend distributing roles amongst the group, maybe looking at different variables or different presentation formats.\nYou’re welcome to use the project time however you’d like. Below is a rough guide if you’re unsure:\n\n\n\n\n\n\n\nSession\nRecommendation\n\n\n\n\nTuesday afternoon\nPick a dataset and play with the data, consider dividing roles amongst group\n\n\nWednesday morning\nMore exploratory analysis, start collecting initial visualisations\n\n\nWednesday afternoon\nPreparing format (e.g. dashboard), continue analysing and creating visualisation\n\n\nThursday morning\nPolish up results and finalise dashboard, begin merging work across group\n\n\nThursday afternoon\nFinish up dashboard and upload\n\n\n\nThe presentations will be during Thursday’s final session at 2:50pm.\nGood luck!",
    "crumbs": [
      "Workshops",
      "The Project",
      "Overview"
    ]
  },
  {
    "objectID": "Projects/details.html#design-and-submissions",
    "href": "Projects/details.html#design-and-submissions",
    "title": "Overview",
    "section": "Design and submissions",
    "text": "Design and submissions\nWe’ve put together an example dashboard which you’re welcome to start from. Download the code to have a look at our example. You can also see a gallery of examples on Quarto’s website.\n\nUploading the submission\nBelow are two ways you can upload your visualisations.\nYou should consider where your dashboard gets its data from. Our repo has all datasets in the data_sources folder, so you have two options:\n\nInclude the data in your dashboard’s folder that you upload.\nUse the relative reference ../../data_sources/data_set_of_your_choice.csv to access the data on our repo\n\nWe’ve used the second option in the example project.\nWhen you are ready to upload, you have two options:\n\nUpload dashboard to GitHub\nThis is the advanced way of doing things - we recommend this way, because it’ll show you a good insight into using GitHub.\n\nCreate a GitHub account and log in\nCreate a fork of our python-training-intensive repository. This is a copy of the repo in your account, changes will not automatically affect the main repo. Leave all settings unchanged.\nCreate a folder for your dashboard inside the Projects directory\n\nGo into the Projects folder, and click Add file \\(\\rightarrow\\) Create new file. This will be your folder.\nGive it a name but no content.\nPress Commit changes....\n\nUpload your files\n\nGo into your new folder and click Add file \\(\\rightarrow\\) Upload files.\nUpload your dashboard and associated files.\nPress Commit changes.\n\nMerge your repo with ours\n\nPress &lt;&gt; Code (top left) to go back to the top level of the repo\nPress Contribute \\(\\rightarrow\\) Open pull request to request merging your files into the main repo\nPress Create pull request when you’re ready.\nIf you need to make further changes, no worries - the pull request will stay up to date with these until approved or closed.\n\n\nGive it a go if you can!\nIf there’s any issues, we’ll leave a comment for you to fix up the merge and we’ll approve it when ready.\n\n\nUpload dashboard to Teams\nIf you’re having issues with the GitHub approach, we can upload it for you. Just put your folder with the dashboard into our Teams folder.",
    "crumbs": [
      "Workshops",
      "The Project",
      "Overview"
    ]
  },
  {
    "objectID": "Workshops/index.html",
    "href": "Workshops/index.html",
    "title": "Workshops",
    "section": "",
    "text": "Over these three days we’ll cover six sessions of content:\n\n\n\nSession\nDescription\n\n\n\n\nThe Fundamentals\nThe basics of Python. Variables, functions and modules.\n\n\nData processing\nImporting, manipulating and analysing data with pandas\n\n\nVisualisation\nCreating visualisations of our data with seaborn, matplotlib and plotly\n\n\nSharing and Publishing\nUsing GitHub for sharing and version control, as well as quarto for publishing dashboards and websites.\n\n\nStatistics\nDescriptive and inferential statistics, with some regressions and hypothesis testing, using scipy.stats and statsmodels\n\n\nProgramming Essentials\nPython tools everyone should know. Conditionals, loops, functions and importing scripts.\n\n\n\nThese content sessions are pretty packed, and we won’t have too much time to deviate. That’s why we’ll also have five project sessions - see The Project for details. You’re welcome to ask lengthier questions and play around there!",
    "crumbs": [
      "Workshops"
    ]
  },
  {
    "objectID": "Workshops/5 - Statistics.html",
    "href": "Workshops/5 - Statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "This session is aimed as an overview of how to perform some statistical modelling with Python. It is a Python workshop, not a statistics workshop - if you’d like to better understand the statistical models, or need help deciding what’s best for you, please consult a statistics resource or contact a statistician.\nIn this session, we’ll cover\nWe’ll use three new modules: - numpy - scipy.stats - statsmodels\nWe’ll be working from our “Players2024” dataset again. To bring it in and clean it up,\nimport pandas as pd\n\ndf = pd.read_csv(\"data/Players2024.csv\")\ndf = df[df[\"positions\"] != \"Missing\"]\ndf = df[df[\"height_cm\"] &gt; 100]",
    "crumbs": [
      "Workshops",
      "Statistics"
    ]
  },
  {
    "objectID": "Workshops/5 - Statistics.html#descriptive-statistics",
    "href": "Workshops/5 - Statistics.html#descriptive-statistics",
    "title": "Statistics",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nWe’ll start with sample size. All dataframes have most descriptive statistics functions available right off the bat which we access via the . operator.\nTo calculate the number of non-empty observations in a column, say the numeric variable df[\"height_cm\"], we use the .count() method\n\ndf[\"height_cm\"].count()\n\nnp.int64(5932)\n\n\n\nMeasures of central tendancy\nWe can compute measures of central tendancy similarly. The average value is given by\n\ndf[\"height_cm\"].mean()\n\nnp.float64(183.04130141604855)\n\n\nthe median by\n\ndf[\"height_cm\"].median()\n\nnp.float64(183.0)\n\n\nand the mode by\n\ndf[\"height_cm\"].mode()\n\n0    185.0\nName: height_cm, dtype: float64\n\n\n\n.mode() returns a dataframe with the most frequent values as there can be multiple.\n\n\n\nMeasures of variance\nWe can also compute measures of variance. The minimum and maximum are as expected\n\ndf[\"height_cm\"].min()\ndf[\"height_cm\"].max()\n\nnp.float64(206.0)\n\n\nThe range is the difference\n\ndf[\"height_cm\"].min() - df[\"height_cm\"].max()\n\nnp.float64(-46.0)\n\n\nQuantiles are given by .quantile(...) with the fraction inside. The inter-quartile range (IQR) is the difference between 25% and 75%.\n\nq1 = df[\"height_cm\"].quantile(0.25)\nq3 = df[\"height_cm\"].quantile(0.75)\nIQR = q3 - q1\n\nA column’s standard deviation and variance are given by\n\ndf[\"height_cm\"].std()\ndf[\"height_cm\"].var()\n\nnp.float64(46.7683158241558)\n\n\nAnd the standard error of the mean (SEM) with\n\ndf[\"height_cm\"].sem()\n\nnp.float64(0.08879229764682213)\n\n\nYou can calculate the skewness and kurtosis (variation of tails) of a sample with\n\ndf[\"height_cm\"].skew()\ndf[\"height_cm\"].kurt()\n\nnp.float64(-0.4338044567190438)\n\n\nAll together, you can see a nice statistical summary with\n\ndf[\"height_cm\"].describe()\n\ncount    5932.000000\nmean      183.041301\nstd         6.838736\nmin       160.000000\n25%       178.000000\n50%       183.000000\n75%       188.000000\nmax       206.000000\nName: height_cm, dtype: float64\n\n\n\n\nMeasures of correlation\nIf you’ve got two numeric variables, you might want to examine covariance and correlation. These indicate how strongly the variables are linearly related. We’ll need to use the df[\"age\"] variable as well.\nThe covariance between “height_cm” and “age” is\n\ndf[\"height_cm\"].cov(df[\"age\"])\n\nnp.float64(0.5126608276592355)\n\n\n\nThe .cov() function compares the column it’s attached to (here df[\"height_cm\"]) with the column you input (here df[\"age\"]). This means we could swap the columns without issue:\n{python} df[\"age\"].cov(df[\"height_cm\"])\n\nSimilarly, we can find the Pearson correlation coefficient between two columns.\n\ndf[\"height_cm\"].corr(df[\"age\"])\n\nnp.float64(0.01682597901197298)\n\n\nYou can also specify “kendall” or “spearman” for their respective correlation coefficients\n\ndf[\"height_cm\"].corr(df[\"age\"], method = \"kendall\")\ndf[\"height_cm\"].corr(df[\"age\"], method = \"spearman\")\n\nnp.float64(0.007604345289158663)\n\n\n\n\nReminder about groupbys\nBefore we move to inferential statistics, it’s worth reiterating the power of groupbys discussed in the second workshop.\nTo group by a specific variable, like “positions”, we use\n\ngb = df.groupby(\"positions\")\n\nBy applying our statistics to the gb object, we’ll apply them to every variable for each position. Note that we should specify numeric_only = True, because these statistics won’t work for non-numeric variables\n\ngb.mean(numeric_only = True)\n\n\n\n\n\n\n\n\nheight_cm\nage\n\n\npositions\n\n\n\n\n\n\nAttack\n180.802673\n25.061108\n\n\nDefender\n184.193269\n25.716471\n\n\nGoalkeeper\n190.668508\n26.587017\n\n\nMidfield\n180.497017\n25.201671",
    "crumbs": [
      "Workshops",
      "Statistics"
    ]
  },
  {
    "objectID": "Workshops/5 - Statistics.html#inferential-statistics",
    "href": "Workshops/5 - Statistics.html#inferential-statistics",
    "title": "Statistics",
    "section": "Inferential Statistics",
    "text": "Inferential Statistics\nInferential statistics requires using the module scipy.stats, which we’ll bring in with\n\nimport scipy.stats as stats\n\n\nSimple linear regressions\nLeast-squares regression for two sets of measurements can be performed with the function stats.linregress()”\n\nstats.linregress(x = df[\"age\"], y = df[\"height_cm\"])\n\nLinregressResult(slope=np.float64(0.025827494764561896), intercept=np.float64(182.38260451315895), rvalue=np.float64(0.01682597901197298), pvalue=np.float64(0.19506275453364344), stderr=np.float64(0.019930266529602007), intercept_stderr=np.float64(0.5159919571772644))\n\n\nIf we store this as a variable, we can access the different values with the . operator. For example, the p-value is\n\nlm = stats.linregress(x = df[\"age\"], y = df[\"height_cm\"])\nlm.pvalue\n\nnp.float64(0.19506275453364344)\n\n\n\nPlotting it\nNaturally, you’d want to plot this. We’ll need to use the overlaying techniques from the visualisation session. Let’s import seaborn and matplotlib\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nStart by making a scatterplot of the data,\n\nsns.relplot(data = df, x = \"age\", y = \"height_cm\")\n\n\n\n\n\n\n\n\nThen, you’ll need to plot the regression as a line. For reference,\n\\[ y = \\text{slope}\\times x + \\text{intercept}\\]\nSo\n\nsns.relplot(data = df, x = \"age\", y = \"height_cm\")\n\nx_lm = df[\"age\"]\ny_lm = lm.slope*x_lm + lm.intercept\nsns.lineplot(x = x_lm, y = y_lm, color = \"r\")\n\n\n\n\n\n\n\n\n\n\n\n\\(t\\)-tests\nWe can also perform \\(t\\)-tests with the scipy.stats module. Typically, this is performed to examine the statistical signficance of a difference between two samples’ means. Let’s examine whether that earlier groupby result for is accurate for heights, specifically, are goalkeepers taller than non-goalkeepers?\nLet’s start by separating the goalkeepers from the non-goalkeepers in two variables\n\ngoalkeepers = df[df[\"positions\"] == \"Goalkeeper\"]\nnon_goalkeepers = df[df[\"positions\"] != \"Goalkeeper\"]\n\nThe \\(t\\)-test for the means of two independent samples is given by\n\nstats.ttest_ind(goalkeepers[\"height_cm\"], non_goalkeepers[\"height_cm\"])\n\nTtestResult(statistic=np.float64(35.2144964816995), pvalue=np.float64(7.551647917141636e-247), df=np.float64(5930.0))\n\n\nYielding a p-value of \\(8\\times 10^{-247}\\approx 0\\), indicating that the null-hypothesis (heights are the same) is extremely unlikely.\n\n\nANOVAs\nWhat about the means of the other three? We could use an ANOVA to examine them. We use the stats.f_oneway() function for this. However, this requires us to send a list of samples in for each group, so we should separate the three positions.\n\ndefender = df[df[\"positions\"] == \"Defender\"].height_cm\nmidfield = df[df[\"positions\"] == \"Midfield\"].height_cm\nattack = df[df[\"positions\"] == \"Attack\"].height_cm\n\nWe can then perform the ANOVA on this list of samples\n\nstats.f_oneway(defender, midfield, attack)\n\nF_onewayResult(statistic=np.float64(199.74794987909772), pvalue=np.float64(2.6216423274516227e-84))\n\n\nWith \\(p = 3\\times10^{-84}\\), it looks like their positions are not all independent of height.\n\n\n\\(\\chi^2\\) tests\n\\(χ^2\\) tests are useful for examining the relationship of categorical variables by comparing the frequencies of each. Often, you’d use this if you can make a contingency table.\nWe only have one useful categorical variable here, “positions” (the others have too many unique values), so we’ll need to create another. Let’s see if there’s a relationship between players’ positions and names with the letter “a”.\nMake a binary column for players with the letter “a” in their names. To do this, we need to apply a string method to all the columns in the dataframe as follows\n\ndf[\"a_in_name\"] = df[\"name\"].str.contains(\"a\")\n\nLet’s cross tabulate positions with this new column\n\na_vs_pos = pd.crosstab(df[\"positions\"],df[\"a_in_name\"])\nprint(a_vs_pos)\n\na_in_name   False  True \npositions               \nAttack        291   1280\nDefender      355   1606\nGoalkeeper    149    575\nMidfield      312   1364\n\n\nThe \\(χ^2\\) test’s job is to examine whether players’ positions depend on the presence of “a” in their name. To evaluate it we need to send the contingency table in:\n\nstats.chi2_contingency(a_vs_pos)\n\nChi2ContingencyResult(statistic=np.float64(2.1808405074930404), pvalue=np.float64(0.5357320466340114), dof=3, expected_freq=array([[ 293.17211733, 1277.82788267],\n       [ 365.9519555 , 1595.0480445 ],\n       [ 135.10923803,  588.89076197],\n       [ 312.76668914, 1363.23331086]]))\n\n\n\n\nMore complex modelling\nIf you need to do more advanced statistics, particularly if you need more regressions, you’ll likely need to turn to a different package: statsmodels. It is particularly useful for statistical modelling.\nWe’ll go through three examples\n\nSimple linear regressions (like before)\nMultiple linear regressions\nLogistic regressions\n\nWhat’s nice about statsmodels is that it gives an R-like interface and summaries.\nTo start with, let’s import the tools. We’ll use the formula interface, which offers us an R-like way of creating models.\n\nimport statsmodels.formula.api as smf\n\n\nSimple linear regressions revisited\nLet’s perform the same linear regression as before, looking at the “age” and “height variables”. Our thinking is that players’ heights dictate how long they can play, so we’ll make \\(x = \\text{height\\_cm}\\) and \\(y = \\text{age}\\).\nThe first step is to make the set up the variables. We’ll use the function smf.ols() for ordinary least squares. It takes in two imputs:\n\nThe formula string, in the form y ~ X1 + X2 ...\nThe data\n\nWe create the model and compute the fit\n\nmod = smf.ols(\"age ~ height_cm\", df)\nres = mod.fit()\n\nDone! Let’s take a look at the results\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nage\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n1.679\n\n\nDate:\nWed, 16 Apr 2025\nProb (F-statistic):\n0.195\n\n\nTime:\n06:00:21\nLog-Likelihood:\n-17279.\n\n\nNo. Observations:\n5932\nAIC:\n3.456e+04\n\n\nDf Residuals:\n5930\nBIC:\n3.457e+04\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n23.4973\n1.549\n15.165\n0.000\n20.460\n26.535\n\n\nheight_cm\n0.0110\n0.008\n1.296\n0.195\n-0.006\n0.028\n\n\n\n\n\n\n\n\nOmnibus:\n204.256\nDurbin-Watson:\n0.269\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n206.613\n\n\nSkew:\n0.427\nProb(JB):\n1.36e-45\n\n\nKurtosis:\n2.671\nCond. No.\n4.91e+03\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 4.91e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\nThat’s a lot nicer than with scipy. We can also make a plot by getting the model’s \\(y\\) values with res.fittedvalues\n\nsns.relplot(data = df, x = \"height_cm\", y = \"age\")\nsns.lineplot(x = df[\"height_cm\"], y = res.fittedvalues, color = \"black\")\n\n\n\n\n\n\n\n\n\n\nGeneralised linear models\nThe statsmodels module has lots of advanced statistical models available. We’ll take a look at one more: Generalised Linear Models. The distributions they include are\n\nBinomial\nPoisson\nNegative Binomial\nGaussian (Normal)\nGamma\nInverse Gaussian\nTweedie\n\nWe’ll use the binomial option to create logistic regressions.\nLogistic regressions examine the distribution of binary data. For us, we can compare the heights of goalkeepers vs non-goalkeepers again. Let’s make a new column which is 1 for goalkeepers and 0 for non-goalkeepers:\n\ndf[\"gk\"] = (df[\"positions\"] == \"Goalkeeper\")*1\n\n\nMultiplying by 1 turns True \\(\\rightarrow\\) 1 and False \\(\\rightarrow\\) 0\n\nNow, we can model this column with height. Specifically,\n\\[ \\text{gk} \\sim \\text{height\\_cm}\\]\nStart by making the model with the function smf.glm(). We need to specify the family of distributions; they all live in sm.families, which comes from a different submodule that we should import:\n\nimport statsmodels.api as sm\nmod = smf.glm(\"gk ~ height_cm\", data = df, family = sm.families.Binomial())\n\nNext, evaluate the results\n\nres = mod.fit()\n\nLet’s have a look at the summary:\n\nres.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ngk\nNo. Observations:\n5932\n\n\nModel:\nGLM\nDf Residuals:\n5930\n\n\nModel Family:\nBinomial\nDf Model:\n1\n\n\nLink Function:\nLogit\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-1583.5\n\n\nDate:\nWed, 16 Apr 2025\nDeviance:\n3167.0\n\n\nTime:\n06:00:22\nPearson chi2:\n4.02e+03\n\n\nNo. Iterations:\n7\nPseudo R-squ. (CS):\n0.1879\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-53.2336\n1.927\n-27.622\n0.000\n-57.011\n-49.456\n\n\nheight_cm\n0.2745\n0.010\n26.938\n0.000\n0.255\n0.294\n\n\n\n\n\nFinally, we can plot the result like before\n\nsns.relplot(data = df, x = \"height_cm\", y = \"gk\")\nsns.lineplot(x = df[\"height_cm\"], y = res.fittedvalues, color = \"black\")",
    "crumbs": [
      "Workshops",
      "Statistics"
    ]
  },
  {
    "objectID": "Workshops/3 - Visualisation.html",
    "href": "Workshops/3 - Visualisation.html",
    "title": "Visualisation",
    "section": "",
    "text": "In this third workshop we will cover",
    "crumbs": [
      "Workshops",
      "Visualisation"
    ]
  },
  {
    "objectID": "Workshops/3 - Visualisation.html#setting-up",
    "href": "Workshops/3 - Visualisation.html#setting-up",
    "title": "Visualisation",
    "section": "Setting up",
    "text": "Setting up\nWith the data manipulation tools from pandas, we can now visualise our data. For this workshop we’ll be working from the “Players2024.csv” dataset, which we should bring in with pandas:\n\nimport pandas as pd\ndf = pd.read_csv(\"data/Players2024.csv\")\n\nTake a quick peak at the dataset to remind yourself\n\nprint(df)\n\n                        name  birth_date  height_cm   positions nationality  \\\n0               James Milner  1986-01-04      175.0    Midfield     England   \n1        Anastasios Tsokanis  1991-05-02      176.0    Midfield      Greece   \n2              Jonas Hofmann  1992-07-14      176.0    Midfield     Germany   \n3                 Pepe Reina  1982-08-31      188.0  Goalkeeper       Spain   \n4              Lionel Carole  1991-04-12      180.0    Defender      France   \n...                      ...         ...        ...         ...         ...   \n5930  Oleksandr Pshenychnyuk  2006-05-01      180.0    Midfield     Ukraine   \n5931            Alex Marques  2005-10-23      186.0    Defender    Portugal   \n5932             Tomás Silva  2006-05-25      175.0    Defender    Portugal   \n5933             Fábio Sambú  2007-09-06      180.0      Attack    Portugal   \n5934          Hakim Sulemana  2005-02-19      164.0      Attack       Ghana   \n\n      age                                    club  \n0      38  Brighton and Hove Albion Football Club  \n1      33        Volou Neos Podosferikos Syllogos  \n2      32             Bayer 04 Leverkusen Fußball  \n3      42                             Calcio Como  \n4      33                      Kayserispor Kulübü  \n...   ...                                     ...  \n5930   18              ZAO FK Chornomorets Odessa  \n5931   18                  Boavista Futebol Clube  \n5932   18                  Boavista Futebol Clube  \n5933   17                  Boavista Futebol Clube  \n5934   19                    Randers Fodbold Club  \n\n[5935 rows x 7 columns]",
    "crumbs": [
      "Workshops",
      "Visualisation"
    ]
  },
  {
    "objectID": "Workshops/3 - Visualisation.html#seaborn-for-simple-visualisations",
    "href": "Workshops/3 - Visualisation.html#seaborn-for-simple-visualisations",
    "title": "Visualisation",
    "section": "Seaborn for simple visualisations",
    "text": "Seaborn for simple visualisations\nTo begin our visualisations, we’ll use the package seaborn, which allows you to quickly whip up decent graphs.\n\nimport seaborn as sns\n\n\nIt’s called “seaborn” as a reference to fictional character Sam Seaborn, whose initials are “sns”.\n\nSeaborn has three plotting functions\n\nsns.catplot(...) # for categorical plotting, e.g. bar plots, box plots etc.\nsns.relplot(...) # for relational plotting, e.g. line plots, scatter plots\nsns.displot(...) # for distributions, e.g. histograms\n\nWe’ll begin with the first.\n\nCategorical plots\nCategorical plots are produced with seaborn’s sns.catplot() function. There are two key pieces of information to pass:\n\nThe data\nThe variables\n\nLet’s see if there’s a relationship between the players’ heights and positions, by placing their positions on the \\(x\\) axis and heights on the \\(y\\).\n\nsns.catplot(data = df, x = \"positions\", y = \"height_cm\")\n\n\n\n\n\n\n\n\nOur first graph! This is called a swarm plot; it’s like a scatter plot for categorical variables.\nIt’s already revealed two things to us about the data:\n\nThere are some incorrect heights - nobody is shorter than 25cm!\nSomeone’s position is “missing”\n\nLet’s get rid of these with the data analysis techniques from last session\n\n# Remove missing position\ndf = df[df[\"positions\"] != \"Missing\"]\n\n# Ensure reasonable heights\ndf = df[df[\"height_cm\"] &gt; 100]\n\nRun the plot again, it’s more reasonable now\n\nsns.catplot(data = df, x = \"positions\", y = \"height_cm\")\n\n\n\n\n\n\n\n\n\nBar plots\nSwarm plots are interesting but not standard. You can change the plot type with the kind parameter\n\nsns.catplot(data = df, x = \"positions\", y = \"height_cm\", kind = \"bar\")\n\n\n\n\n\n\n\n\n\nMany aspects of your plot can be adjusted by sending in additional parameters and is where seaborn excels.\n\nIt seems like goalkeepers are taller, but not by much. Let’s look at the standard deviation for each position by changing the estimator = parameter (default is mean)\n\nsns.catplot(data = df, x = \"positions\", y = \"height_cm\", kind = \"bar\", estimator = \"std\")\n\n\n\n\n\n\n\n\nClearly there’s a lot less variation in goalkeepers - they’re all tall.\n\n\nBox plots\nLet’s make box plots instead. It’s the same procedure, just change to kind = \"box\" and remove estimator =\n\nsns.catplot(data = df, x = \"positions\", y = \"height_cm\", kind = \"box\")\n\n\n\n\n\n\n\n\nJust as we predicted.\n\n\n\nDistributions\n\nHistograms\nLet’s move to the “Age” parameter now. We can look at the distribution of ages with\n\nsns.displot(data = df, x = \"age\")\n\n\n\n\n\n\n\n\nLooks a bit funny with those gaps - let’s change the number of bins with bins = 28\n\nsns.displot(data = df, x = \"age\", bins = 28)\n\n\n\n\n\n\n\n\nNow, what if you wanted to look at the distribution for different variables? We can make a separate distribution for each position with the col = \"positions\" argument, specifying a new column for each position\n\nsns.displot(data = df, x = \"age\", bins = 28, col = \"positions\")\n\n\n\n\n\n\n\n\n\n\nKernel density estimates\nFinally, you don’t have to do histograms. You could also do a Kernel Density Estimate, with kind = \"kde\" (let’s remove bins = and col =)\n\nsns.displot(data = df, x = \"age\", kind = \"kde\")\n\n\n\n\n\n\n\n\nIf you want a separate line for each position, we should indicate that each position needs a different colour/hue with hue = \"positions\"\n\nsns.displot(data = df, x = \"age\", hue = \"positions\", kind = \"kde\")\n\n\n\n\n\n\n\n\n\n\n\nRelational plots\nIt seems like players peak in their mid-twenties, but goalkeepers stay for longer. Let’s see if there’s a relationship between players’ age and height\n\nScatter plots\nWe’ll start with a scatter plot\n\nsns.relplot(data = df, x = \"age\", y = \"height_cm\")\n\n\n\n\n\n\n\n\nNot much of a trend there, although the bottom-right looks a bit emptier than the rest (could it be that short old players are the first to retire?).\nWe can use hue = to have a look at positions again\n\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", hue = \"positions\")\n\n\n\n\n\n\n\n\nYup, goalkeepers are tall, and everyone else is a jumble.\n\n\nLine plots\nLet’s do a line plot of the average height per age.\n\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", kind = \"line\")\n\n\n\n\n\n\n\n\nSeems pretty flat, except the ends are a bit weird because there’s not much data. Let’s eliminate everything before 17 and after 38 and plot it\n\n# Create smaller dataframe\ncondition = (df[\"age\"] &gt; 17) & (df[\"age\"] &lt; 38)\ninner_ages = df[condition]\n\n# Line plot\nsns.relplot(data = inner_ages, x = \"age\", y = \"height_cm\", kind = \"line\")\n\n\n\n\n\n\n\n\nLooks a bit shaky but that’s just because it’s zoomed in - notice that we go from 182cm to 184cm. We’ll fix this when we look at matplotlib in the next section.\n\n\nCombining the two\nWe can combine our scatter and line plots together.\n\nMake the first plot as normal\nFor all additional (overlaying) plots, use an axes-level plot instead of sns.relplot() etc. These just draw the points/bars/lines, and are normally behind-the-scenes. There’s one for every plot type, and look like\n\nsns.lineplot()\nsns.scatterplot()\nsns.boxplot()\nsns.histplot()\netc.\n\n\nFor example,\n\n# Figure level plot\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", hue = \"positions\")\n\n# Axes level plot (drop the kind = )\nsns.lineplot(data = inner_ages, x = \"age\", y = \"height_cm\")\n\n\n\n\n\n\n\n\n\nYou can’t include kind = inside an axes level plot\n\nLet’s swap the colour variable from the scatter plot to the line plot\n\n# Figure level plot\nsns.relplot(data = df, x = \"age\", y = \"height_cm\")\n\n# Axes level plot (drop the kind = )\nsns.lineplot(data = inner_ages, x = \"age\", y = \"height_cm\", hue = \"positions\")\n\n\n\n\n\n\n\n\nFinally, let’s make the scatter dots smaller with s = 10 and grey with color = \"grey\".\n\n# Figure level plot\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", s = 10, color = \"grey\")\n\n# Axes level plot (drop the kind = )\nsns.lineplot(data = inner_ages, x = \"age\", y = \"height_cm\", hue = \"positions\")",
    "crumbs": [
      "Workshops",
      "Visualisation"
    ]
  },
  {
    "objectID": "Workshops/3 - Visualisation.html#going-deeper-with-matplotlib",
    "href": "Workshops/3 - Visualisation.html#going-deeper-with-matplotlib",
    "title": "Visualisation",
    "section": "Going deeper with matplotlib",
    "text": "Going deeper with matplotlib\nSeaborn is great for simple and initial visualisations, but when you need to make adjustments it gets tricky. At its core, seaborn is just a simple way of using matplotlib, an extensive and popular plotting package. It was created as a way of doing MATLAB visualisations with Python, so if you’re coming from there, things will feel familiar.\nPros\n\nCustomisable. You can tweak almost every parameter of the visualisations\nFast. It can handle large data\nPopular. Lots of people use it, and knowing it will help you collaborate\n\nCons - a bit programmy\n\nSteep-ish learning curve. Creating basic plots can be easy, but its set up with enough complexity that it takes a bit of work to figure out what’s going on.\nCumbersome. You can tweak almost everything, but this means that it can take some effort to tweak anything.\n\nWe’re barely going to touch the matplotlib surface, but we’ll look at some essentials.\nTo begin with, we want to bring in matplotlib as follows\n\nimport matplotlib.pyplot as plt\n\n\nSaving plots\nBefore we move to adjusting the plot, let’s just look at how you save it. While you can do this with seaborn, the matplotlib way is also very simple.\nAs a first step, you should make a new folder. Navigate using your file explorer to the project and create a new folder called “plots”.\nNext, save the current plot with plt.savefig(\"place_location_here\"), and we have to do this at the same time that we make the plot. So run all this code at once:\n\nplt.savefig(\"plots/first_saved_plot.png\")\n\n\n\nMaking modifications\n\nTitles\nNotice that the \\(y\\) axis has an ugly label? That’s because seaborn is just drawing from your dataframe.\nWe can change axis labels with plt.ylabel()\n\n# Plotting functions\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", s = 10, color = \"grey\")\nsns.lineplot(data = inner_ages, x = \"age\", y = \"height_cm\", hue = \"positions\")\n\n# Customisation\nplt.ylabel(\"Height (cm)\")\n\nText(4.8166666666666655, 0.5, 'Height (cm)')\n\n\n\n\n\n\n\n\n\nand similarly you could change plt.xlabel(...).\n\nMake sure you run the above line at the same time as your plotting function. You can either * Highlight all the code and press F9 * Make a cell with #%% and press ctrl + enter\n\nWe can also change the legend title to “positions” with plt.legend()\n\n# Plotting functions\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", s = 10, color = \"grey\")\nsns.lineplot(data = inner_ages, x = \"age\", y = \"height_cm\", hue = \"positions\")\n\n# Customisation\nplt.ylabel(\"Height (cm)\")\nplt.legend(title = \"positions\")\n\n\n\n\n\n\n\n\nAnd its location with loc = \"lower left\"\n\n# Plotting functions\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", s = 10, color = \"grey\")\nsns.lineplot(data = inner_ages, x = \"age\", y = \"height_cm\", hue = \"positions\")\n\n# Customisation\nplt.ylabel(\"Height (cm)\")\nplt.legend(title = \"positions\")\n\n\n\n\n\n\n\n\nAnd give the whole plot a title with plt.title()\n\n# Figure level plot\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", s = 10, color = \"grey\")\n\n# Axes level plot (drop the kind = )\nsns.lineplot(data = inner_ages, x = \"age\", y = \"height_cm\", hue = \"positions\")\n\n# Titles\nplt.ylabel(\"Height (cm)\")\nplt.legend(title = \"positions\")\nplt.title(\"Players' heights vs ages\")\n\nText(0.5, 1.0, \"Players' heights vs ages\")\n\n\n\n\n\n\n\n\n\n\n\n\nAnnotations\nYou might want to annotate your plot with text and arrows. Text is simple with the plt.text() function; we just need to specify its coordinates and the contents.\n\n# Figure level plot\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", s = 10, color = \"grey\")\n\n# Axes level plot (drop the kind = )\nsns.lineplot(data = inner_ages, x = \"age\", y = \"height_cm\", hue = \"positions\")\n\n# Titles\nplt.ylabel(\"Height (cm)\")\nplt.legend(title = \"positions\")\nplt.title(\"Players' heights vs ages\")\n\n# Annotations\nplt.text(38.5, 181, \"Not enough\\ndata for mean\")\n\nText(38.5, 181, 'Not enough\\ndata for mean')\n\n\n\n\n\n\n\n\n\n\nThe characters \\n mean ‘new line’\n\nWe could annotate with arrows too. This is more complex, using the plt.annotate() function:\n\n# Figure level plot\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", s = 10, color = \"grey\")\n\n# Axes level plot (drop the kind = )\nsns.lineplot(data = inner_ages, x = \"age\", y = \"height_cm\", hue = \"positions\")\n\n# Titles\nplt.ylabel(\"Height (cm)\")\nplt.legend(title = \"positions\")\nplt.title(\"Players' heights vs ages\")\n\n# Annotations\nplt.text(38.5, 181, \"Not enough\\ndata for mean\")\nplt.annotate(text = \"No short\\nolder players\", xy = [37,165], xytext = [40,172],\n             arrowprops = dict(width = 1, headwidth = 10, headlength = 10, \n                          facecolor = \"black\"))\n\nText(40, 172, 'No short\\nolder players')\n\n\n\n\n\n\n\n\n\n\nI’ve split this over multiple lines, but its still one function - check the brackets\n\nAll together, our plot has become\n\n\nAxis limits\nThe last feature we’ll look at is editing axis limits. Let’s try to make more room in the bottom left for the legend with the functions plt.xlim() and plt.ylim()\n\n# Figure level plot\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", s = 10, color = \"grey\")\n\n# Axes level plot (drop the kind = )\nsns.lineplot(data = inner_ages, x = \"age\", y = \"height_cm\", hue = \"positions\")\n\n# Titles\nplt.ylabel(\"Height (cm)\")\nplt.legend(title = \"positions\", loc = \"lower left\")\nplt.title(\"Players' heights vs ages\")\n\n# Annotations\nplt.text(38.5, 181, \"Not enough\\ndata for mean\")\nplt.annotate(\"No short\\nolder players\", [37,165], [40,172], \n             arrowprops = dict(width = 1,headwidth = 10,headlength = 10, \n                               facecolor = \"black\"))\n\n# Axis limits\nplt.xlim([10,45])\nplt.ylim([150,210])\n\n\n\n\n\n\n\n\nI’m not sure that looks any better, but you get the idea!",
    "crumbs": [
      "Workshops",
      "Visualisation"
    ]
  },
  {
    "objectID": "Workshops/3 - Visualisation.html#interactivity-with-plotly",
    "href": "Workshops/3 - Visualisation.html#interactivity-with-plotly",
    "title": "Visualisation",
    "section": "Interactivity with plotly",
    "text": "Interactivity with plotly\nFor the last part of this section, we’re going to briefly look at making interactive plots with plotly.\nWe bring in the tools with\n\nimport plotly.express as px\n\n\nYou’ll probably need to install it first - use either\nconda install plotly\nOR\npip install plotly\ndepending on your installation.\n\nPlotly works by creating a visualisation like we’ve been doing, and then loading it into something dynamic, like a web browser. Spyder does not support interactive plots. This means we need to change the default settings with\n\nimport plotly.io as pio\npio.renderers.default = \"browser\"\n\nNow, plots should all load in your default browser.\n\nThe basics\nWe make plotly graphs very similarly to seaborn. Let’s take our first plot from above,\n\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", s = 10, color = \"grey\")\n\n\n\n\n\n\n\n\nand turn it into a plotly one.\n\nWe need to use px.scatter instead of sns.relplot\nWe need to use data_frame = instead of data =\nLet’s remove the s = and color = for now\nSave the plot as a variable\n\n\npx.scatter(data_frame = df, x = \"age\", y = \"height_cm\")\n\n        \n        \n        \n\n\n                            \n                                            \n\n\nNotice how you can hover over the points now? It’s interactive!\n\n\nIntroducing more info and neatening up\nLike seaborn’s “hue”, we can use color = to introduce a third variable\n\npx.scatter(data_frame = df, x = \"age\", y = \"height_cm\", color = \"positions\")\n\n                            \n                                            \n\n\nAnd like seaborn’s “col”, we can facet with facet_col =\n\npx.scatter(data_frame = df, x = \"age\", y = \"height_cm\", color = \"positions\",\n           facet_col = \"positions\")\n\n                            \n                                            \n\n\nPersonally, I think these are too squished. We can specify the maximum number of columns with facet_col_wrap =\n\npx.scatter(data_frame = df, x = \"age\", y = \"height_cm\", color = \"positions\",\n           facet_col = \"positions\", facet_col_wrap = 2)\n\n                            \n                                            \n\n\nFinally, let’s adjust the information in the hover. We can give each point a name with hover_name = - how about their actual names?\n\npx.scatter(data_frame = df, x = \"age\", y = \"height_cm\", color = \"positions\",\n           facet_col = \"positions\", facet_col_wrap = 2, hover_name = \"name\")\n\n                            \n                                            \n\n\nAnd let’s also include their nationalities\n\npx.scatter(data_frame = df, x = \"age\", y = \"height_cm\", color = \"positions\",\n           facet_col = \"positions\", facet_col_wrap = 2, hover_name = \"name\",\n           hover_data = \"nationality\")\n\n                            \n                                            \n\n\n\n\nSaving interactive plots\nSince these are interactive, we can’t save them as normal. The easiest option is to save them as HTML files - like websites - which we can open from our browsers.\nFirst, save the plot into a variable\n\nfig = px.scatter(data_frame = df, x = \"age\", y = \"height_cm\", color = \"positions\",\n                 facet_col = \"positions\", facet_col_wrap = 2, hover_name = \"name\",\n                 hover_data = \"nationality\")\n\nThen, write it to HTML\n\nfig.write_html(\"plot.html\")",
    "crumbs": [
      "Workshops",
      "Visualisation"
    ]
  },
  {
    "objectID": "Workshops/1 - Fundamentals.html",
    "href": "Workshops/1 - Fundamentals.html",
    "title": "The Fundamentals",
    "section": "",
    "text": "In this first workshop we will cover",
    "crumbs": [
      "Workshops",
      "The Fundamentals"
    ]
  },
  {
    "objectID": "Workshops/1 - Fundamentals.html#introducing-python-and-spyder",
    "href": "Workshops/1 - Fundamentals.html#introducing-python-and-spyder",
    "title": "The Fundamentals",
    "section": "Introducing Python and Spyder",
    "text": "Introducing Python and Spyder\nPython is a programming language that can be used to build programs (i.e. a “general programming language”), but it can also be used to analyse data by importing a number of useful modules.\nWe are using Spyder to interact with Python more comfortably. If you have used RStudio to interact with R before, you should feel right at home: Spyder is a program designed for doing data science with Python.\nPython can be used interactively in a console, or we can build scripts and programs with it, making the most out of Spyder’s code editor.\nWe will start by using the console to work interactively. This is our direct line to the computer, and is the simplest way to run code. Don’t worry about any unfamiliar language, fonts or colours - we can ignore most of it for now - all you need to know is that\n\nIn [1]: ... is code that we’ve sent to the computer, and\nOut[1]: ... is its response.",
    "crumbs": [
      "Workshops",
      "The Fundamentals"
    ]
  },
  {
    "objectID": "Workshops/1 - Fundamentals.html#first-glance-arithmetic",
    "href": "Workshops/1 - Fundamentals.html#first-glance-arithmetic",
    "title": "The Fundamentals",
    "section": "First glance: arithmetic",
    "text": "First glance: arithmetic\nTo start with, we can use Python like a calculator. Type the following commands in the console, and press Enter to execute them:\n\n1 + 1\n\n2\n\n\n\n2 * 3\n\n6\n\n\n\n4 / 10\n\n0.4\n\n\n\n5 ** 2\n\n25\n\n\nAfter running each command, you should see the result as an output.",
    "crumbs": [
      "Workshops",
      "The Fundamentals"
    ]
  },
  {
    "objectID": "Workshops/1 - Fundamentals.html#variables",
    "href": "Workshops/1 - Fundamentals.html#variables",
    "title": "The Fundamentals",
    "section": "Variables",
    "text": "Variables\nLike language, Python has nouns and verbs. We call the nouns variables: they are the ‘things’ we manipulate with our code.\nEssentially, a variable is a named container. We access it by its name, and we get its value.\nTo create a variable, you need to choose a name and a value with name = value. For example\n\nexample_int = 42\n\nWhenever you use the variable’s name, Python will now access its value:\n\nexample_int\n\n42\n\n\nWe can use the variables in place of the values\n\nexample_float = 5.678\nproduct = example_int * example_float\nproduct\n\n238.476\n\n\n\nSpyder helps us with extra panels and features apart from the Console. To see what variables you have created, look at the “Variable explorer” tab in the top right.\n\n\nTypes\nVariables have different types. So far, we’ve just looked at storing numbers, of which there are three types:\n\nint - integers store whole numbers, e.g. 1, 5, 1000, -3.\nfloat - floating point numbers store decimals and scientific notation, e.g. 1.5, -8.97, 4e-6.\ncomplex - complex numbers express the imaginary unit with j, e.g. z = 1+2j is \\(z = 1+2i\\).\n\nLet’s look at some other types\n\nBooleans\nEven simpler than integers is the boolean type. These are either 1 or 0 (True or False), representing a single binary unit (bit). Don’t be fooled by the words, these work like numbers: True + True gives 2.\n\nexample_bool = True\n\n\nIn Python, the boolean values True and False must begin with a capital letter.\n\n\n\nStrings\nLet’s look at variable types which aren’t (necessarily) numbers. Sequences are variables which store multiple pieces of data. For example, strings store a sequence of characters and are created with quotation marks 'blah blah blah' or \"blah blah blah\":\n\nexample_string = 'This is an example of a string!'\n\n\n\nLists\nWe can also create lists, which will store several variables (not necessarily of the same type). We need to use square brackets for that:\n\nexample_numbers = [38, 3, 54, 17, 7]\nexample_diverse = [3, 'Hi!', 9.0]\n\nLists are very flexible as they can contain any number of items, and any type of data. You can even nest lists inside a list, which makes for a very flexible data type.\nOperations on sequences are a bit different to numbers. We can still use + and *, but they will concatenate (append) and duplicate, rather than perform arithmetic.\n\nexample_string + ' How are you?'\nexample_numbers + example_diverse\n3 * example_numbers\n\n[38, 3, 54, 17, 7, 38, 3, 54, 17, 7, 38, 3, 54, 17, 7]\n\n\nHowever, depending on the variable, some operations won’t work:\n\nexample_string + example_int\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 example_string + example_int\n\nTypeError: can only concatenate str (not \"int\") to str\n\n\n\nThere are other data types like tuples, dictionaries and sets, but we won’t look at those in this session. Here’s a summary of the ones we’ve covered:\n\n\n\n\n\n\n\n\n\n\nCategory\nType\nShort name\nExample\nGenerator\n\n\n\n\nNumeric\nInteger\nint\n3\nint()\n\n\nNumeric\nFloating Point Number\nfloat\n4.2\nfloat()\n\n\nNumeric\nBoolean\nbool\nTrue\nbool()\n\n\nSequence\nString\nstr\n'A sentence '\n\" \" or ' ' or str()\n\n\nSequence\nList\nlist\n['apple', 'banana', 'cherry']\n[ ] or list()\n\n\n\nThe generator commands are new. We use these to manually change the variable type. For example,\n\nint(True)\n\n1\n\n\nyields 1, converting a boolean into an integer. These commands are functions, as opposed to variables - we’ll look at functions a bit later.\n\n\n\nIndexing\nWe can access part of a sequence by indexing. Sequences are ordered, starting at 0, so the first element has index 0, the second index 1, the third 2 and so on. For example, see what these commands return:\n\nexample_string[0]\nexample_string[6]\nexample_numbers[4]\n\n7\n\n\nIf you want more than one element in a sequence, you can slice. Simple slices specify a range to slice, from the first index to the last, but not including the last. For example:\n\nexample_numbers[0:4]\n\n[38, 3, 54, 17]\n\n\nThat command returns elements from position 0 up to - but not including! - position 4.",
    "crumbs": [
      "Workshops",
      "The Fundamentals"
    ]
  },
  {
    "objectID": "Workshops/1 - Fundamentals.html#scripts",
    "href": "Workshops/1 - Fundamentals.html#scripts",
    "title": "The Fundamentals",
    "section": "Scripts",
    "text": "Scripts\nSo far, we’ve been working in the console, our direct line to the computer. However, it is often more convenient to use a script. These are simple text files which store code and run when we choose. They are useful to\n\nwrite code more comfortably,\nstore clearly defined steps in chronological order,\nshare a process with peers easily, and\nmake your work reproducible\n\nLet’s create a folder system to store our script in by creating a project.\n\nPress Projects &gt; New project... and name your project, perhaps “python_training”.\nCreate a new script with ctrl+N, File &gt; New file... or the new file button.\n\nYou should now see a script on the left panel in Spyder, looking something like this:\nTry typing a line of code in your new script, such as\n\nexample_message = \"This is an example message\"\nexample_message\n\n'This is an example message'\n\n\nPress F9 to run each line, or ctrl+enter for the whole script. You should see something like the following appear in the console (depending on how you ran it):\nWe’ll work out of a script for the rest of the session. Don’t forget to save your script by pressing ctrl+S or the save button. )",
    "crumbs": [
      "Workshops",
      "The Fundamentals"
    ]
  },
  {
    "objectID": "Workshops/1 - Fundamentals.html#functions",
    "href": "Workshops/1 - Fundamentals.html#functions",
    "title": "The Fundamentals",
    "section": "Functions",
    "text": "Functions\nFunctions are little programs that do specific jobs. These are the verbs of Python, because they do things to and with our variables. Here are a few examples of built-in functions:\n\nlen(example_numbers)\nmin(example_numbers)\nmax(example_numbers)\nsum(example_numbers)\nround(example_float)\n\n6\n\n\nFunctions always have parentheses () after their name, and they can take one or several arguments, or none at all, depending on what they can do, and how the user wants to use them.\nHere, we use two arguments to modify the default behaviour of the round() function:\n\nround(example_float, 2)\n\n5.68\n\n\n\nNotice how Spyder gives you hints about the available arguments after typing the function name?\n\n\nOperators\nOperators are a special type of function in Python with which you’re already familiar. The most important is =, which assigns values to variables. Here is a summary of some important operators, although there are many others:\n\nGeneral\n\n\n\n\n\n\n\n\n\nOperator\nFunction\nDescription\nExample command\n\n\n\n\n=\nAssignment\nAssigns values to variables\na = 7\n\n\n#\nComment\nExcludes any following text from being run\n# This text will be ignored by Python\n\n\n\n\n\nMathematical\n\n\n\n\n\n\n\n\n\n\nOperator\nFunction\nDescription\nExample command\nExample output\n\n\n\n\n+\nAddition\nAdds or concatenates values, depending on variable types\n7 + 3 or \"a\" + \"b\"\n10 or 'ab'\n\n\n-\nSubtraction\nSubtracts numerical values\n8 - 3\n5\n\n\n*\nMultiplication\nMultiplies values, depending on variable types\n7 * 2 or \"a\" * 3\n14 or 'aaa'\n\n\n**\nExponentiation\nRaises a numerical value to a power\n7 ** 2\n49\n\n\n/\nDivision\nDivides numerical values\n3 / 4\n0.75\n\n\n//\nFloor division\nDivides numerical values and then rounds down\n3 // 4\n0\n\n\n%\nRemainder\nTakes the remainder of numerical values\n13 % 7\n6\n\n\n\n\n\nComparison\n\n\n\n\n\n\n\n\n\n\nOperator\nFunction\nDescription\nExample command\nExample output\n\n\n\n\n==\nEqual to\nChecks whether two variables are the same and outputs a boolean\n1 == 1\nTrue\n\n\n!=\nNot equal to\nChecks whether two variables are different\n'1' != 1\nTrue\n\n\n&gt;\nGreater than\nChecks whether one variable is greater than the other\n1 &gt; 1\nFalse\n\n\n&gt;=\nGreater than or equal to\nChecks whether greater than (&gt;) or equal to (==) are true\n1 &gt;= 1\nTrue\n\n\n&lt;\nLess than\nChecks whether one variable is less than the other\n0 &lt; 1\nTrue\n\n\n&lt;=\nLess than or equal to\nChecks whether less than (&lt;) or equal to (==) are true\n0 &lt;= 1\nTrue",
    "crumbs": [
      "Workshops",
      "The Fundamentals"
    ]
  },
  {
    "objectID": "Workshops/1 - Fundamentals.html#finding-help",
    "href": "Workshops/1 - Fundamentals.html#finding-help",
    "title": "The Fundamentals",
    "section": "Finding help",
    "text": "Finding help\nTo find help about a function, you can use the help() function, or a ? after a function name:\n\nhelp(max)\nprint?\n\nHelp on built-in function max in module builtins:\n\nmax(...)\n    max(iterable, *[, default=obj, key=func]) -&gt; value\n    max(arg1, arg2, *args, *[, key=func]) -&gt; value\n    \n    With a single iterable argument, return its biggest item. The\n    default keyword-only argument specifies an object to return if\n    the provided iterable is empty.\n    With two or more arguments, return the largest argument.\n\n\n\nIn Spyder, you can use the Ctrl + I keyboard shortcut to open the help in a separate pane.\nFor a comprehensive manual, go to the official online documentation. For questions and answers, typing the right question in a search engine will usually lead you to something helpful. If you can’t find an answer, StackOverflow is a great Q&A community.",
    "crumbs": [
      "Workshops",
      "The Fundamentals"
    ]
  },
  {
    "objectID": "Workshops/1 - Fundamentals.html#modules",
    "href": "Workshops/1 - Fundamentals.html#modules",
    "title": "The Fundamentals",
    "section": "Modules",
    "text": "Modules\nPython, on its own, requires a lot of manual programming for advanced tasks. What makes it versatile is the capacity to use other people’s code with modules.\nTo bring in advanced variables and functions that other’s have made, we need to import the module. For example\n\npi\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[20], line 1\n----&gt; 1 pi\n\nNameError: name 'pi' is not defined\n\n\n\nreturns an error, because it’s undefined. But the math module contains a variable called pi:\n\nimport math\nmath.pi\n\n3.141592653589793\n\n\n\nTo access objects from within a module, we use a full stop: module.object_inside.\n\n\nNumPy for arrays\nArrays are a data type introduced by numpy, a module with many functions useful for numerical computing.\nFor example, you can convert the list we created before to then do mathematical operations on each one of its elements:\n\nimport numpy as np\nexample_array = np.array(example_numbers)\nexample_array * 2\n\narray([ 76,   6, 108,  34,  14])\n\n\n\n\nPandas for dataframes\npandas introduces dataframes, which are often used to store two-dimensional datasets with different kinds of variables in each column. If your data is stored as a spreadsheet, you probably want to import it with a pandas function.\nFirst, we’ll need to install and import the pandas module. Install it as before (either with pip or conda), and then run\n\nimport pandas as pd\n\nNow, let’s import some data to get ready for the next session.\n\nCreate a new folder in your project called data\nDownload the Players2024.csv file\nMove it into your new data folder\nUse the function pd.read_csv() to load the data:\n\n\ndf = pd.read_csv(\"data/Players2024.csv\")\n\nYou’ll see that it’s now in your variable explorer. You can double-click on a dataframe in the Variable explorer to explore it in a separate window.\nWe’ll look at manipulating these dataframe objects in the next session. For now, try running the df.head() function to examine the first few rows:\n\ndf.head()\n\n\n\n\n\n\n\n\nname\nbirth_date\nheight_cm\npositions\nnationality\nage\nclub\n\n\n\n\n0\nJames Milner\n1986-01-04\n175.0\nMidfield\nEngland\n38\nBrighton and Hove Albion Football Club\n\n\n1\nAnastasios Tsokanis\n1991-05-02\n176.0\nMidfield\nGreece\n33\nVolou Neos Podosferikos Syllogos\n\n\n2\nJonas Hofmann\n1992-07-14\n176.0\nMidfield\nGermany\n32\nBayer 04 Leverkusen Fußball\n\n\n3\nPepe Reina\n1982-08-31\n188.0\nGoalkeeper\nSpain\n42\nCalcio Como\n\n\n4\nLionel Carole\n1991-04-12\n180.0\nDefender\nFrance\n33\nKayserispor Kulübü\n\n\n\n\n\n\n\n\n\nMatplotlib for visualisation\nmatplotlib is a large collection of data visualisation functions, and pyplot is a submodule of matplotlib that contains essentials.\n\nimport matplotlib.pyplot as plt\nplt.plot(example_array)\n\n\n\n\n\n\n\n\nThis shows a plot in the Plots tab of Spyder.\n\nIn a Python shell, you might have to use the plt.show() command to show the plot.\n\nThe default look is a line plot that joins all the points, but we can style a plot with only a few characters:\n\n# blue circles\nplt.plot(example_array, 'bo')\n\n# green squares, dashed line:\nplt.plot(example_array, 'gs--')\n\n\n\n\n\n\n\n\nExtra arguments can be used to style further:\n\n# red, diamonds, solid line; change width of line and size of diamonds:\nplt.plot(example_array, 'rd-', linewidth=3, markersize=10)\n\n\n\n\n\n\n\n\nTo find out about the styling shorthand and all other arguments, look at the documentation:\n\nplt.plot?\n\n\n\nInstalling modules that aren’t built in\nThe math module is built-in - the module came when I installed Python, and the numpy, pandas and matplotlib come with conda installations. Most other modules live online, so we need to download and install them first.\nInstalling modules depends on whether you have a conda environment or not. To check, run\n\nconda\n\n\n\n\n\n\n\n\nMessage\nconda Environment?\n\n\n\n\nconda is a tool for managing and deployi... or something similar\nYes\n\n\nNameError: name 'conda' is not defined\nNo\n\n\n\n\nIf you have a conda environment\nYou can install packages with\n\nconda install package_name\n\n\nYou likely have a conda environment if you installed Anaconda or you installed Spyder 6 (Since Oct 2024)\n\n\n\nIf you do not have a conda environment\nYou can install packages with\n\npip install package_name\n\n\nYou likely have a pip environment if you installed Python manually or are using an older (before Oct 2024) version of Spyder (e.g. Spyder 5)\n\n\n\n\nPlotly Express for interactive visualisations\nOne module that isn’t built-in is plotly, which we can use for interactive visualisations.\n\nimport plotly.io as pio\nimport plotly.express as px\n\n# Set renderer\npio.renderers.default='browser'\n\n# Create bar plot\npx.histogram(df, x = \"age\", color = \"positions\")\n\n\n---------------------------------------------------------------------------\nError                                     Traceback (most recent call last)\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/IPython/core/formatters.py:984, in IPythonDisplayFormatter.__call__(self, obj)\n    982 method = get_real_method(obj, self.print_method)\n    983 if method is not None:\n--&gt; 984     method()\n    985     return True\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/plotly/basedatatypes.py:833, in BaseFigure._ipython_display_(self)\n    830 import plotly.io as pio\n    832 if pio.renderers.render_on_display and pio.renderers.default:\n--&gt; 833     pio.show(self)\n    834 else:\n    835     print(repr(self))\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/plotly/io/_renderers.py:434, in show(fig, renderer, validate, **kwargs)\n    431     ipython_display.display(bundle, raw=True)\n    433 # external renderers\n--&gt; 434 renderers._perform_external_rendering(fig_dict, renderers_string=renderer, **kwargs)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/plotly/io/_renderers.py:369, in RenderersConfig._perform_external_rendering(self, fig_dict, renderers_string, **kwargs)\n    366     if hasattr(renderer, k):\n    367         setattr(renderer, k, v)\n--&gt; 369 renderer.render(fig_dict)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/plotly/io/_base_renderers.py:729, in BrowserRenderer.render(self, fig_dict)\n    714 from plotly.io import to_html\n    716 html = to_html(\n    717     fig_dict,\n    718     config=self.config,\n   (...)\n    727     validate=False,\n    728 )\n--&gt; 729 open_html_in_browser(html, self.using, self.new, self.autoraise)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/plotly/io/_base_renderers.py:644, in open_html_in_browser(html, using, new, autoraise)\n    641 browser = None\n    643 if using is None:\n--&gt; 644     browser = webbrowser.get(None)\n    645 else:\n    646     if not isinstance(using, tuple):\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/webbrowser.py:65, in get(using)\n     63         elif command[0] is not None:\n     64             return command[0]()\n---&gt; 65 raise Error(\"could not locate runnable browser\")\n\nError: could not locate runnable browser",
    "crumbs": [
      "Workshops",
      "The Fundamentals"
    ]
  },
  {
    "objectID": "Workshops/1 - Fundamentals.html#saving-your-work",
    "href": "Workshops/1 - Fundamentals.html#saving-your-work",
    "title": "The Fundamentals",
    "section": "Saving your work",
    "text": "Saving your work\nPress “Save” or Ctrl + S to save your script.\nYour project can be reopened from the “Projects” menu in Spyder.\nBy default, your variables are not saved, which is another reason why working with a script is important: you can execute the whole script in one go to get everything back. You can however save your variables as a .spydata file if you want to (for example, if it takes a lot of time to process your data).",
    "crumbs": [
      "Workshops",
      "The Fundamentals"
    ]
  },
  {
    "objectID": "Workshops/1 - Fundamentals.html#summary",
    "href": "Workshops/1 - Fundamentals.html#summary",
    "title": "The Fundamentals",
    "section": "Summary",
    "text": "Summary\nThis morning we looked at a lot of Python features, so don’t worry if they haven’t all sunk in. Programming is best learned through practice, so keep at it! Here’s a rundown of the concepts we covered\n\n\n\n\n\n\n\nConcept\nDesctiption\n\n\n\n\nThe console vs scripts\nThe console is our window into the computer, this is where we send code directly to the computer. Scripts are files which we can write, edit, store and run code, that’s where you’ll write most of your Python.\n\n\nVariables\nVariables are the nouns of programming, this is where we store information, the objects and things of our coding. They come in different types like integers, strings and lists.\n\n\nIndexing\nIn order to access elements of a sequence variable, like a list, we need to index, e.g. example_numbers[2]. Python counts from 0.\n\n\nFunctions\nFunctions are the verbs of programming, they perform actions on our variables. Call the function by name and put inputs inside parentheses, e.g. round(2.5)\n\n\nHelp\nRunning help( ... ) will reveal the help documentation about a function or type.\n\n\nPackages\nWe can bring external code into our environment with import .... This is how we use packages, an essential for Python. Don’t forget to install the package first!",
    "crumbs": [
      "Workshops",
      "The Fundamentals"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Intro and Setting Up",
    "section": "",
    "text": "Welcome to our three-day Python training intensive! By Thursday afternoon, you’ll have learnt the Python skills to manipulate, visualise and present data. We’ll spend roughly half the time learning content, and half the time working on a project in groups.\nAs we set up, there’s a few things to do, if you haven’t already\n\nInstall the software\nIntroduce yourself to your table\nJoin our Teams channel\nRegister your attendance",
    "crumbs": [
      "Workshops",
      "Intro and Setting Up"
    ]
  },
  {
    "objectID": "setup.html#overview",
    "href": "setup.html#overview",
    "title": "Intro and Setting Up",
    "section": "",
    "text": "Welcome to our three-day Python training intensive! By Thursday afternoon, you’ll have learnt the Python skills to manipulate, visualise and present data. We’ll spend roughly half the time learning content, and half the time working on a project in groups.\nAs we set up, there’s a few things to do, if you haven’t already\n\nInstall the software\nIntroduce yourself to your table\nJoin our Teams channel\nRegister your attendance",
    "crumbs": [
      "Workshops",
      "Intro and Setting Up"
    ]
  },
  {
    "objectID": "setup.html#software",
    "href": "setup.html#software",
    "title": "Intro and Setting Up",
    "section": "Software",
    "text": "Software\nWe are going to use Spyder for writing and running Python. This is a friendly interactive development environment (IDE) aimed at researchers. However, you are more than welcome to use your own!\nPlease set up Python and your IDE in advance. If you don’t have Python or an IDE, we recommend installing Spyder, which comes with Python.\nWe’ll also be using the rendering and publishing tool Quarto from day 2, so please download and install Quarto.\n\nGoogle Colab\nIf you aren’t able to install Python and a suitable IDE on your device (e.g. if you do not have permission) then we can find an online alternative for you, likely in the form of Google Colab. Let us know and we’ll help you get set up!",
    "crumbs": [
      "Workshops",
      "Intro and Setting Up"
    ]
  },
  {
    "objectID": "setup.html#creating-a-project",
    "href": "setup.html#creating-a-project",
    "title": "Intro and Setting Up",
    "section": "Creating a Project",
    "text": "Creating a Project\nIf you’re using Spyder, we recommend you create a project. Projects are just fancy folders, and they make it easier to access files (i.e. data) and export images (i.e. visualisations) all in the one place.\n\nOpen Spyder\nIn the Projects menu, click New Project…\nChoose New Directory, give your project a name and find an appropriate place on your computer for it.\nPress Create\n\nDone! We’ll work in this project for the duration of the intensives.",
    "crumbs": [
      "Workshops",
      "Intro and Setting Up"
    ]
  },
  {
    "objectID": "Workshops/2 - Data processing.html",
    "href": "Workshops/2 - Data processing.html",
    "title": "Data Processing",
    "section": "",
    "text": "In this second workshop we will cover\nThis hands-on course – directed at intermediate users – looks at using the pandas module to transform and visualise tabular data.",
    "crumbs": [
      "Workshops",
      "Data Processing"
    ]
  },
  {
    "objectID": "Workshops/2 - Data processing.html#setting-up",
    "href": "Workshops/2 - Data processing.html#setting-up",
    "title": "Data Processing",
    "section": "Setting up",
    "text": "Setting up\n\nIntroducing pandas\nPandas is a Python module that introduces dataframes to Python. It gives us the tools we need to clean and transform data with Python.\nTo be able to use the functions included in pandas, we have to first import it:\n\nimport pandas as pd\n\npd is the usual nickname for the pandas module.\n\nIf you get an error, like No module named 'pandas', you’ll need to install it first, using either conda install pandas or pip install pandas, depending on your Python installation.\n\n\nThe DataFrame object\nPandas is built upon one key feature: the DataFrame class. In Python we have different built-in types, like int for integers and string for characters. Pandas introduces a new type, DataFrame, which stores data like a spreadsheet.\n\n\n\nSetting up the workspace\nTo make life easy, we should set up our workspace well.\n\nOpen your project folder using your file explorer, and create a new folder called “data”.\nDownload the data for today’s session\nMove the file into your new “data” folder\nNext, open your project in Spyder, and create a new script called “analysis.py”.\nOpen the “Files” tab in Spyder and check that you see two objects:\n\nThe file “analysis.py”\nThe folder “data”\n\n\n\n\nImporting data\nPandas offers a simple way to access data with its read.csv() function. We’ll save it into the variable df_raw:\n\ndf_raw = pd.read_csv(\"data/Players2024.csv\")\n\n\nYou can also provide a URL instead of a file path!\n\n\n\nAside - File Paths and backslashes\nJust a quick detour to discuss file paths of which there are two types: absolute and relative\n\nAbsolute\nAbsolute file paths always start at the “top” of your file system, e.g. one of the drives (like C:) for Windows users, so they are never ambiguous. It’s like providing your full address from country to street number.\nC://Users/my_username/research/data/really_important_secret_data.csv\n\n\nRelative\nRelative file paths start from your current working directory, which is usually the top folder of a Spyder project. For files in my current folder, I just provide their name - like referring to another house on your street as “number 7”. Let’s assume we’re in the “research” folder.\nfile_in_my_current_folder.csv\nWe can go to down folders from our current location:\ndata/really_important_secret_data.csv\nAnd we can go up folders from our current location\n../../this_file_is_two_levels_up.csv\nOr a combination of the two (e.g. up one, then down into a different folder)\n../not_research/this_file_is_not_research.csv\nWhat matters is that the relative reference depends on where your code is and will break if you move the script!\n\n\nBackslashes\nOne last note: Windows uses backslashes for their file paths\nC:\\\\Users\\...\nBut Python uses backslashes as an escape character. For example, \"\\n\" is a newline, \"\\u1234\" is the unicode character U+1234 and confusingly \"\\\\\" is a single backslash. The easist way to get around this is by prefixing r to all strings: this makes them raw.\n\nwindows_url = r\"C:\\\\Users\\...\"\n\n\n\n\nInitial look at the data\nLet’s get back to data.\nWe can investigate the size of the data thanks to the shape attribute attached to all pandas dataframes:\n\ndf_raw.shape\n\n(5935, 7)\n\n\nThe dataset contains dozens of columns. What are their names?\n\ndf_raw.columns\n\nIndex(['name', 'birth_date', 'height_cm', 'positions', 'nationality', 'age',\n       'club'],\n      dtype='object')\n\n\nLet’s subset our data to focus on a handful of variables.\n\n\nCreating a backup\nData analysis in Python is safe because our variables are copies of the data - we aren’t actually changing the files until we explicitly overwrite them. However, Python also has no undo, so if I delete something in my analysis, I can’t get it back - I have to start all over again.\nOne way to mitigate this issue is by making a copy of the data\n\ndf = df_raw.copy()\n\nNow we have two variables: df is what we’ll use, and df_raw stores the raw data. If we ever need to restart, we can simply run df = df_raw.copy().",
    "crumbs": [
      "Workshops",
      "Data Processing"
    ]
  },
  {
    "objectID": "Workshops/2 - Data processing.html#accessing-and-filtering-data",
    "href": "Workshops/2 - Data processing.html#accessing-and-filtering-data",
    "title": "Data Processing",
    "section": "Accessing and Filtering Data",
    "text": "Accessing and Filtering Data\nSo how do we access our data in Python? We use a type of indexing introduced by pandas, which revolves around using square brackets after the dataframe: df[...].\n\nAccessing columns\nTo access a column, index with its name: df[\"column_name\"]. For example,\n\ndf[\"name\"]\n\n0                 James Milner\n1          Anastasios Tsokanis\n2                Jonas Hofmann\n3                   Pepe Reina\n4                Lionel Carole\n                 ...          \n5930    Oleksandr Pshenychnyuk\n5931              Alex Marques\n5932               Tomás Silva\n5933               Fábio Sambú\n5934            Hakim Sulemana\nName: name, Length: 5935, dtype: object\n\n\nreturns the “name” column. We can access multiple by providing a list of names\n\n# Save the names in a list and then index\ncolumn_names = [\"name\", \"club\"]\ndf[column_names]\n\n# This is equivalent to\ndf[[\"name\", \"club\"]]\n\n\n\n\n\n\n\n\nname\nclub\n\n\n\n\n0\nJames Milner\nBrighton and Hove Albion Football Club\n\n\n1\nAnastasios Tsokanis\nVolou Neos Podosferikos Syllogos\n\n\n2\nJonas Hofmann\nBayer 04 Leverkusen Fußball\n\n\n3\nPepe Reina\nCalcio Como\n\n\n4\nLionel Carole\nKayserispor Kulübü\n\n\n...\n...\n...\n\n\n5930\nOleksandr Pshenychnyuk\nZAO FK Chornomorets Odessa\n\n\n5931\nAlex Marques\nBoavista Futebol Clube\n\n\n5932\nTomás Silva\nBoavista Futebol Clube\n\n\n5933\nFábio Sambú\nBoavista Futebol Clube\n\n\n5934\nHakim Sulemana\nRanders Fodbold Club\n\n\n\n\n5935 rows × 2 columns\n\n\n\nIf we want to do anything with it (like statistics or visualisation), it’s worth saving the column(s) as a new variable\n\ndf_subset = df[[\"name\", \"club\"]]\n\n\n\nAccessing rows\nThere’s a few ways to access rows. The easiest is by slicing, df[start_row : end_row]. For example, if you want rows 5 to 10,\n\ndf[5 : 10]\n\n\n\n\n\n\n\n\nname\nbirth_date\nheight_cm\npositions\nnationality\nage\nclub\n\n\n\n\n5\nLudovic Butelle\n1983-04-03\n188.0\nGoalkeeper\nFrance\n41\nStade de Reims\n\n\n6\nDaley Blind\n1990-03-09\n180.0\nDefender\nNetherlands\n34\nGirona Fútbol Club S. A. D.\n\n\n7\nCraig Gordon\n1982-12-31\n193.0\nGoalkeeper\nScotland\n41\nHeart of Midlothian Football Club\n\n\n8\nDimitrios Sotiriou\n1987-09-13\n185.0\nGoalkeeper\nGreece\n37\nOmilos Filathlon Irakliou FC\n\n\n9\nAlessio Cragno\n1994-06-28\n184.0\nGoalkeeper\nItaly\n30\nAssociazione Calcio Monza\n\n\n\n\n\n\n\n\nNote that the end row is not included\n\nIf you want to access a single row, we need to use df.loc[] or df.iloc[]. These are the go-to methods for accessing data if the above indexing isn’t sufficient.\n\ndf.loc[] accesses rows by label (defaults to row number but could be anything)\ndf.iloc[] accesses rows by row number exclusively\n\nBy default they line up, so\n\ndf.loc[5]\ndf.iloc[5]\n\nname           Ludovic Butelle\nbirth_date          1983-04-03\nheight_cm                188.0\npositions           Goalkeeper\nnationality             France\nage                         41\nclub            Stade de Reims\nName: 5, dtype: object\n\n\nare often (but not always) the same.\nFinally, we can filter specific rows by a condition on one of the variables, e.g. only rows where variable \\(\\text{age} &gt; 25\\).\n\ndf[df[\"age\"] &gt; 25]\n# Or any other condition\n\n\n\n\n\n\n\n\nname\nbirth_date\nheight_cm\npositions\nnationality\nage\nclub\n\n\n\n\n0\nJames Milner\n1986-01-04\n175.0\nMidfield\nEngland\n38\nBrighton and Hove Albion Football Club\n\n\n1\nAnastasios Tsokanis\n1991-05-02\n176.0\nMidfield\nGreece\n33\nVolou Neos Podosferikos Syllogos\n\n\n2\nJonas Hofmann\n1992-07-14\n176.0\nMidfield\nGermany\n32\nBayer 04 Leverkusen Fußball\n\n\n3\nPepe Reina\n1982-08-31\n188.0\nGoalkeeper\nSpain\n42\nCalcio Como\n\n\n4\nLionel Carole\n1991-04-12\n180.0\nDefender\nFrance\n33\nKayserispor Kulübü\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5155\nLeo Scienza\n1998-09-13\n175.0\nAttack\nBrazil\n26\n1. Fußballclub Heidenheim 1846\n\n\n5236\nMohamed Brahimi\n1998-09-17\n181.0\nAttack\nFrance\n26\nFK Fakel Voronezh\n\n\n5287\nNicolás Marotta\n1996-12-23\n186.0\nDefender\nArgentina\n27\nAthens Kallithea Football Club\n\n\n5471\nDaniel Sosah\n1998-09-21\n179.0\nAttack\nNiger\n26\nFK Kryvbas Kryvyi Rig\n\n\n5478\nEgas Cacintura\n1997-10-29\n174.0\nMidfield\nAngola\n26\nDinamo Makhachkala\n\n\n\n\n2757 rows × 7 columns\n\n\n\nAs with the column case, it’s useful to save this as a variable\n\ndf_filtered = df[df[\"age\"] &gt; 15]",
    "crumbs": [
      "Workshops",
      "Data Processing"
    ]
  },
  {
    "objectID": "Workshops/2 - Data processing.html#basic-statistics",
    "href": "Workshops/2 - Data processing.html#basic-statistics",
    "title": "Data Processing",
    "section": "Basic statistics",
    "text": "Basic statistics\nHow might we perform some basic statistics on our data?\nTo check what kind of data each column is stored as, we can use the dtypes attribute:\n\ndf.dtypes\n\nname            object\nbirth_date      object\nheight_cm      float64\npositions       object\nnationality     object\nage              int64\nclub            object\ndtype: object\n\n\n\nIn general, pandas will bring in numbers with float64 and non-numeric data with object.\n\nThe describe() method is useful for descriptive statistics about our numerical columns:\n\ndf.describe()\n\n\n\n\n\n\n\n\nheight_cm\nage\n\n\n\n\ncount\n5935.000000\n5935.000000\n\n\nmean\n182.986352\n25.501769\n\n\nstd\n7.478313\n4.455595\n\n\nmin\n17.000000\n15.000000\n\n\n25%\n178.000000\n22.000000\n\n\n50%\n183.000000\n25.000000\n\n\n75%\n188.000000\n29.000000\n\n\nmax\n206.000000\n42.000000\n\n\n\n\n\n\n\nHowever, it will only show the two first ones and two last ones. We can focus on a specific column instead, for example one that was hidden previously:\n\ndf[\"age\"].describe()\n\ncount    5935.000000\nmean       25.501769\nstd         4.455595\nmin        15.000000\n25%        22.000000\n50%        25.000000\n75%        29.000000\nmax        42.000000\nName: age, dtype: float64\n\n\nOr a categorical column:\n\ndf[\"nationality\"].describe()\n\ncount      5935\nunique      135\ntop       Spain\nfreq        402\nName: nationality, dtype: object\n\n\n\nFor a categorical column, the information shown is different: for example, how many unique values there are, and what the most common value is.\n\nWhat if you want specific statistics about a particular column? Usually there are methods available:\n\n# Applicable to all columns\ndf[\"nationality\"].count()\ndf[\"nationality\"].unique()\n\n# For numeric columns only\ndf[\"height_cm\"].min()\ndf[\"height_cm\"].max()\ndf[\"height_cm\"].mean()\ndf[\"height_cm\"].median()\ndf[\"height_cm\"].std()\n# ...\n\nnp.float64(7.478312588515905)\n\n\nWe can use these methods to filter our data. For example, the row which has the maximum value of variable \\(x\\) is\n\nx_max = df[\"height_cm\"].max()\ndf[df[\"height_cm\"] == x_max]\n\n# Or in one line\ndf[df[\"height_cm\"] == df[\"height_cm\"].max()]\n\n\n\n\n\n\n\n\nname\nbirth_date\nheight_cm\npositions\nnationality\nage\nclub\n\n\n\n\n4179\nKevin Gadellaa\n2003-04-08\n206.0\nGoalkeeper\nNetherlands\n21\nFootball Club Utrecht\n\n\n4810\nIsaak Touré\n2003-03-28\n206.0\nDefender\nFrance\n21\nUdinese Calcio\n\n\n5565\nDenys Tvardovskyi\n2003-06-13\n206.0\nGoalkeeper\nUkraine\n21\nFC Shakhtar Donetsk\n\n\n\n\n\n\n\nbecause we are looking for the row in df[\"height_cm\"] (the whole column) that has the value df[\"height_cm\"].max().",
    "crumbs": [
      "Workshops",
      "Data Processing"
    ]
  },
  {
    "objectID": "Workshops/2 - Data processing.html#challenge",
    "href": "Workshops/2 - Data processing.html#challenge",
    "title": "Data Processing",
    "section": "Challenge",
    "text": "Challenge\nReduce your dataset to \\(\\le 3\\) variables (columns) and \\(\\le 100\\) rows using conditions by filtering down to a particular subset of your data.",
    "crumbs": [
      "Workshops",
      "Data Processing"
    ]
  },
  {
    "objectID": "Workshops/2 - Data processing.html#adding-and-removing-columns",
    "href": "Workshops/2 - Data processing.html#adding-and-removing-columns",
    "title": "Data Processing",
    "section": "Adding and removing columns",
    "text": "Adding and removing columns\nSometimes we need to add new columns. It’s the same process as overwriting existing columns - let’s make a new column called “zeroes” where every row is 0\n\ndf[\"zeroes\"] = 0\n\nWe can also send in a column, for example\n\ndf[\"copy_of_names\"] = df[\"name\"]\n\nPerhaps most usefully, we can manipulate the column we send in. For example, the deviation from the mean \\[|\\bar{x} - x_i|\\] can be computed for each row’s height:\n\ncol_x = df[\"height_cm\"]\navg_x = df[\"height_cm\"].mean()\n\ndf[\"deviation_from_mean_height\"] = abs(col_x - avg_x)\n\n# Or all together on one line,\ndf[\"deviation_from_mean_height\"] = abs(df[\"height_cm\"] - df[\"height_cm\"].mean())\n\nwhere abs(...) takes the absolute value\nNotice that we subtracted a value from a column. We can also perform mathematics with multiple columns:\n\ndf[\"product\"] = df[\"age\"]*df[\"height_cm\"]\n\nLet’s remove these new columns that we don’t need with the method df.drop(columns = [...]):\n\ndf = df.drop(columns = [\"zeroes\", \"copy_of_names\", \"deviation_from_mean_height\", \"product\"])",
    "crumbs": [
      "Workshops",
      "Data Processing"
    ]
  },
  {
    "objectID": "Workshops/2 - Data processing.html#summaries",
    "href": "Workshops/2 - Data processing.html#summaries",
    "title": "Data Processing",
    "section": "Summaries",
    "text": "Summaries\nAfter cleaning up our data, we need to analyse it. This usually involves some kind of aggregation. For example, what is the average \\(x\\) per year? requires aggregating over variable \\(x\\) for each year.\nFirst, we need to group by a specific variable\n\ngb = df.groupby(\"age\")\n\nThis thing in itself is a pretty abstract Python object, best thought of as a dataframe where we’ve identified a grouping variable.\nNext, we need to apply some aggregation to it (the groupby tells it to do it for each year)\n\navg_height_by_age = gb[\"height_cm\"].agg(\"mean\")\n\nOf course, we could have done this in one line:\n\navg_height_by_age = df.groupby(\"age\")[\"height_cm\"].agg(\"mean\")\n\nThis is a really useful tool, because now we have something we can visualise. As the next session will show us, the visualisation tools generally just take in numbers and turn them into dots. We need to do the stats beforehand.\nAs a taster, try running\n\navg_height_by_age.plot()",
    "crumbs": [
      "Workshops",
      "Data Processing"
    ]
  },
  {
    "objectID": "Workshops/2 - Data processing.html#exporting-results",
    "href": "Workshops/2 - Data processing.html#exporting-results",
    "title": "Data Processing",
    "section": "Exporting results",
    "text": "Exporting results\nThe last step in the process is saving the data. Let’s say we want to take that final dataframe and export it to a csv. That’s what the df.to_csv() method is for\n\navg_height_by_age.to_csv(\"data/avg_height_by_age.csv\")\n\nThis will save the dataframe to a .csv file and place it in the data folder.",
    "crumbs": [
      "Workshops",
      "Data Processing"
    ]
  },
  {
    "objectID": "Workshops/2 - Data processing.html#resources",
    "href": "Workshops/2 - Data processing.html#resources",
    "title": "Data Processing",
    "section": "Resources",
    "text": "Resources\n\nOfficial pandas documentation\n\nGetting started\n10 Minutes to pandas\nUser guide\n\nMore visualisation modules:\n\nAltair\nBokeh\nVega\nMatplotlib\n\nOur compilation of useful Python links",
    "crumbs": [
      "Workshops",
      "Data Processing"
    ]
  },
  {
    "objectID": "Workshops/4 - Sharing and Publishing.html",
    "href": "Workshops/4 - Sharing and Publishing.html",
    "title": "Sharing and Publishing",
    "section": "",
    "text": "In this workshop we cover using GitHub for sharing your source code, Git for version control, and Quarto for publishing outputs. Specifically, we look at:",
    "crumbs": [
      "Workshops",
      "Sharing and Publishing"
    ]
  },
  {
    "objectID": "Workshops/4 - Sharing and Publishing.html#quarto",
    "href": "Workshops/4 - Sharing and Publishing.html#quarto",
    "title": "Sharing and Publishing",
    "section": "Quarto",
    "text": "Quarto\nQuarto is a publishing system that allows creating documents, presentations, websites and dashboards that contain prose, code and code outputs. This means that such outputs can detail exactly what happened to the data, and outputs can be re-generated very quickly if, for example, the underlying dataset was updated, or if the analysis needs to change.\n\nInstalling Quarto\nFollow the instructions to install Quarto on your computer. Quarto is a command line tool available for all major operating systems.\n\nWe can write Quarto files in Spyder, but there is (at the time of writing) no integration of it into the Spyder interface. Other IDEs make it easier to interact with Quarto functions and write Quarto files, like R Studio.\n\nOnce Quarto is installed, restart Spyder and try running !quarto version in Spyder’s IPython console. If it returns a version number, Quarto was installed successfully and you will be able to run Quarto commands straight from Spyder (using the ! at the beginning).\nIf the IPython console can’t find Quarto, try to run quarto version (without the !) in a command line interface outside Spyder (bash, macOS’s Terminal, PowerShell…). You will have to use the tool that finds Quarto to run Quarto commands later.\n\n\nCreate a Quarto file\nLet’s try to create a document based on the visualisations we created earlier. Create a new Python script and save it as “document.py” at the top of the project directory.\n\n\n\n\n\n\nWarning\n\n\n\nWhen rendering individual Quarto files, paths to files (like the one you use to import some data) will be understood as relative to the file itself. This means that, to avoid confusion, it is best to save your Quarto file at the top of your project directory, so you can use the same relative paths as in the rest of your scripts.\n\n\nThis new .py script will be made of cells, which can be inserted with the toolbar or the keyboard shortcut Ctrl+2. (You can also write the delimiter # %% by hand if you have an older version of Spyder, or if you find it more convenient.)\nAt the top of our script, we need to include a Markdown cell that contains the document’s settings:\n# %% [markdown]\n# ---\n# title: Reproducible Output\n# author: Your Name\n# date: 2025-01-01\n# ---\nAll subsequent cells are going to be either [markdown] cells (if they contain text, titles and other static contents) or python cells (if they contain executable code). For example:\n# %% [markdown]\n\"\"\"\n## Import the data\n\nLet's import the data:\n\"\"\"\n\n# %%\nimport pandas as pd\ndf = pd.read_csv(\"Players2024.csv\")\n\n# %% [markdown]\n\"\"\"\n## Plot the data\n\nLet's generate that **swarm plot** again:\n\"\"\"\n\n# %%\nimport seaborn as sns\nsns.catplot(data = df, x = \"positions\", y = \"height_cm\")\n\n# %% [markdown]\n\"\"\"\nWe now have a plot!\n\"\"\"\nIn the Markdown cells, we use the Markdown markup language to format the text. For example, using ## before some text defines a heading of level 2 (level 1 being the document’s title), and using ** around some text makes it bold. See more Markdown hints in the Quarto documentation.\n\n\nRendering\nFrom the IPython console, run the following command to render the document:\n!quarto render document.py\nIf you use a different command line tool to run quarto, do not use the ! in front:\nquarto render document.py\nThis should create a HTML document in your project directory, which you can open in a web browser by right-clicking on it in the Files tab and using “Open externally”.\n\nTroubleshooting\nIf the rendering step fails, it might be because some modules are missing. This is more common if Quarto uses a different Python environment to Spyder. Use the right command to install the missing modules (depending on your system, pip install or conda install) and try rendering again.\nIf the issue relates to a missing Python kernel, you might have to install jupyter. See options for different package managers in the Quarto documentation about Python.\n\n\n\nCell options\nAs the default Quarto output is a HTML file, we can include interactive visualisations too.\nLet’s say we also want to let our readers know that they need to install Plotly in order to create interactive visualisations. If you want to show the corresponding code in your document but don’t want to run it, you can add the cell option #| eval: false. (And if you want to show the output but not show the underlying code, use #| echo: false.)\n# %% [markdown]\n\"\"\"\n## Interactive plots\n\nYou will need the plotly module:\n\"\"\"\n\n# %%\n#| eval: false\nconda install plotly\n\n# %% [markdown]\n\"\"\"\nAn interactive plot:\n\"\"\"\n\n# %%\n#| echo: false\nimport plotly.express as px\npx.scatter(data_frame = df, x = \"age\", y = \"height_cm\")\nAnd for adding a caption and alternative text to a figure:\n# %%\n#| fig-cap: \"Goalkeepers tend to be taller.\"\n#| fig-alt: \"A scatterplot of the relationsip between height and position.\"\nimport seaborn as sns\nsns.catplot(data = df, x = \"positions\", y = \"height_cm\")\nMany more cell options exist, including captioning and formatting visualisations. Note that these options can be used at the cell level as well as globally (by modifying the front matter at the top of the document).\nFor example, to make sure error and warning messages are never shown:\n# %% [markdown]\n# ---\n# title: Reproducible Output\n# author: Your Name\n# date: 2025-01-01\n# warning: false\n# error: false\n# ---\n\n\nOutput formats\nThe default output format in Quarto is HTML, which is by far the most flexible. However, Quarto is a very versatile publishing system and can generate many different output formats, including PDF, DOCX and ODT, slide formats, Markdown suited for GitHub… and even whole blogs, books and dashboards.\nLet’s try rendering a PDF:\n# %% [markdown]\n# ---\n# title: Reproducible Output\n# author: Your Name\n# date: 2025-01-01\n# format: pdf\n# ---\nWhen rendering PDFs, the first issue we might run into is the lack of a LaTeX distribution. If Quarto didn’t detect one, it will suggest to install tinytex (a minimal LaTeX distribution) with:\n!quarto install tinytex\nOnce that is installed, Quarto should render a PDF.\nAnother issue with our example document is that an interactive HTML visualisation won’t be renderd in the PDF. You can supress it by using the #| eval: false option:\n# %%\n#| eval: false\nimport plotly.express as px\npx.scatter(data_frame = df, x = \"age\", y = \"height_cm\")\n\nDashboard\nA great way to present a variety of outputs in a grid is by creating a HTML dashboard.\nLet’s modify our script to render a dashboard. First, change the output format:\n# %% [markdown]\n# ---\n# title: Reproducible Output\n# author: Your Name\n# date: 2025-01-01\n# format: dashboard\n# ---\nWe can already render the dashboard and see the result. Each panel can be expanded with the bottom-right button. Note that by default:\n\neach cell is rendered in a separate card\nheadings define the rows\nthe heading text is discarded\ncode is not shown (but can be by using echo: true)\n\nGiven this default behaviour, you might have to rethink a good part of your script to make it suited for a striking dashboard. For example, removing most of the text, customising the layout (tabsets, rows, card heights…) and adding custom cards like “value boxes”. Learn more about all these in the Quarto Dashboards documentation.\nAs a starting point, copy the current script across to a new script called dashboard.py and modify it so it matches the following:\n# %% [markdown]\n# ---\n# title: Reproducible Outputs\n# author: Your Name\n# date: today\n# warning: false\n# error: false\n# format: dashboard\n# ---\n\n# %%\nimport pandas as pd\ndf = pd.read_csv(\"Players2024.csv\")\n\n# %% [markdown]\n\"\"\"\n## Figures {height=70%}\n\"\"\"\n\n# %%\n#| title: Goalkeepers tend to be taller\n#| fig-alt: \"A scatterplot of the relationsip between height and position.\"\nimport seaborn as sns\nsns.catplot(data = df, x = \"positions\", y = \"height_cm\")\n\n# %%\n#| title: Age vs Height\nimport plotly.express as px\npx.scatter(data_frame = df, x = \"age\", y = \"height_cm\")\n\n# %% [markdown]\n\"\"\"\n## Table\n\"\"\"\n\n# %%\n#| title: A glimpse at the dataset\ndf.head(10)\nThis results in a dashboard containing three cards organised in two rows. The top row uses 70% of the available height, and the bottom row shows a table of the top 10 rows of the dataset. Each card has a title.\n\n\n\nQuarto dashboard with three cards: static visualisation, interactive visualisation, and table.\n\n\n\n\n\nThemes\nTo quickly style your dashboard differently, you can use a Bootstrap theme in your header like so:\n# %% [markdown]\n# ---\n# title: Reproducible Outputs\n# author: Your Name\n# date: today\n# warning: false\n# error: false\n# format:\n#   dashboard:\n#     theme: quartz\n# ---\nSee a list of what other themes are available.",
    "crumbs": [
      "Workshops",
      "Sharing and Publishing"
    ]
  },
  {
    "objectID": "Workshops/4 - Sharing and Publishing.html#git-and-github",
    "href": "Workshops/4 - Sharing and Publishing.html#git-and-github",
    "title": "Sharing and Publishing",
    "section": "Git and GitHub",
    "text": "Git and GitHub\nGit is a version control system that allows to record a clean history of your project, track precise authorship, and collaborate asynchronously with others. It can be used offline, from the command line or with integration into Integrated Desktop Environments (like RStudio, VS Code… Unfortunately, Spyder does not have Git integration).\nGitHub is one of many websites that allow you to host project that are tracked with Git. But even without using Git at all, it is possible to use GitHub to share and make your project public. Many researchers use it to make their code public alongside a published paper, to increase reproducibility and transparency. It can also be useful to build and share a portfolio of your work.\nLearning about the ins and out of Git takes time, so in this section we will mainly use GitHub as a place to upload and share your code and outputs, and as a starting point for learning more about Git in the future.\n\nGitHub\nGitHub is currently the most popular place for hosting, sharing and collaborating on code. You can create an account for free, and then create a repository for your project.\n\nCreate an account and log in\nClick on the “+” button (top right of the page)\nSelect “New repository”\nChoose a name and description for your repository\nTick “Add a README file” - this will be where you introduce your project\nClick “Create repository”\n\nFrom there, you can upload your files, and edit text-based files straight from your web browser if you need to.\nThe README file is a markdown file that can contain the most important information about your project. It’s important to populate it as it is the first document most people see. It could contain:\n\nName and description of the project\nHow to use it (installation process if any, examples…)\nWho is the author, who maintains it\nHow to contribute\n\nFor inspiration, see the pandas README file.\nTo practice managing a git repository on GitHub, try creating a personal portfolio repository where you can showcase what you have worked on and the outputs your are most proud of.",
    "crumbs": [
      "Workshops",
      "Sharing and Publishing"
    ]
  },
  {
    "objectID": "Workshops/4 - Sharing and Publishing.html#further-resources",
    "href": "Workshops/4 - Sharing and Publishing.html#further-resources",
    "title": "Sharing and Publishing",
    "section": "Further resources",
    "text": "Further resources\n\nSome alternatives to GitHub: Codeberg and Gitlab\nQuarto documentation\nCourse on Git from the command line\nCourse on Git with GitHub\nHow to use GitHub Pages to publish Quarto outputs",
    "crumbs": [
      "Workshops",
      "Sharing and Publishing"
    ]
  },
  {
    "objectID": "Workshops/6 - Programming.html",
    "href": "Workshops/6 - Programming.html",
    "title": "Programming Essentials",
    "section": "",
    "text": "In this workshop we cover the building blocks for developing more complex code, looking at",
    "crumbs": [
      "Workshops",
      "Programming Essentials"
    ]
  },
  {
    "objectID": "Workshops/6 - Programming.html#directing-traffic-with-conditionals",
    "href": "Workshops/6 - Programming.html#directing-traffic-with-conditionals",
    "title": "Programming Essentials",
    "section": "Directing traffic with conditionals",
    "text": "Directing traffic with conditionals\nIn the first half of this session we’ll look at two types of control flows: conditionals and loops.\nConditionals allow you to put “gates” in your code, only running sections if a certain condition is true. They are common to most programming languages.\nIn Python, they are called if statements, because you use the if command. For example,\n\nif 5 &gt; 0:\n    print(\"We're inside the if statement\")\n\nWe're inside the if statement\n\n\nThe line print(\"We're inside the if statement\") will only run if 5 &gt; 0 is true. If not, it’ll get skipped.\nIndents are essential. Only indented code will be governed by conditional\n\nif 5 &gt; 0:\n    print(\"We're inside the if statement\")\n\nprint(\"This code always runs\")\n\nWe're inside the if statement\nThis code always runs\n\n\nWatch what happens if we change the condition\n\nif 5 &gt; 10:\n    print(\"We're inside the if statement\")\n\nprint(\"This code always runs\")\n\nThis code always runs\n\n\nNow, the first line doesn’t run. That’s the essence of a conditional.\nThere’s not much point to using a condition that will always be true. Typically, you’d use a variable in the condition, for example.\n\npet_age = 10\n\nif pet_age &gt; 10:\n    print(\"My pet is older than 10\")\n\n\nLogical operators\nHere is a table of the different operators you can make conditions with. When you run them, they always return either True or False.\n\n\n\n\n\n\n\n\nOperator\nTrue example\nDescription\n\n\n\n\n==\n10 == 10\nSame value and type\n\n\n!=\n\"10\" != 10\nDifferent value or type\n\n\n&gt;\n10 &gt; 5\nGreater than\n\n\n&gt;=\n10 &gt;= 10\nGreater than or equal to\n\n\n&lt;\n5 &lt; 10\nLess than\n\n\n&lt;=\n5 &lt;= 10\nLess than or equal to\n\n\nin\n\"a\" in \"apple\"\nFirst object exists in the second\n\n\nnot in\n\"b\" not in \"apple\"\nFirst object does not exist in the second\n\n\nand\n10 == 10 and \"a\" in \"apple\"\nOnly true if both conditions are true. Same as &\n\n\nor\n10 == 10 or \"b\" in \"apple\"\nAlways true if one condition is true. Same as \\|\n\n\n\n\n\nelif and else\nif statements only run if the condition is True. What happens if its False? That’s what the else command is for, it’s like a net that catches anything that slipped past if:\n\npet_age = 5\n\nif pet_age &gt; 10:\n    print(\"My pet is older than 10\")\nelse:\n    print(\"My pet is 10 or younger\")\n\nMy pet is 10 or younger\n\n\n\nDon’t forget the colon :!\n\nCheck what happens when you change the age from 5 to 15.\nFinally, what if you wanted to check another condition only if the first one fails? That’s what elif (else-if) is for. It’s another if statement but it only runs if the first fails.\n\npet_age = 5\n\nif pet_age &gt; 10:\n    print(\"My pet is older than 10\")\nelif pet_age &lt; 5:\n    print(\"My pet is younger than 5\")\nelse:\n    print(\"My pet is between 5 and 10\")\n\nMy pet is between 5 and 10\n\n\nYou can include as many as you’d like\n\npet_age = 5\n\nif pet_age &gt; 10:\n    print(\"My pet is older than 10\")\nelif pet_age &lt; 5:\n    print(\"My pet is younger than 5\")\nelif pet_age &lt; 1:\n    print(\"My pet is freshly born\")\nelse:\n    print(\"My pet is between 5 and 10\")\n\nMy pet is between 5 and 10",
    "crumbs": [
      "Workshops",
      "Programming Essentials"
    ]
  },
  {
    "objectID": "Workshops/6 - Programming.html#repeat-after-me",
    "href": "Workshops/6 - Programming.html#repeat-after-me",
    "title": "Programming Essentials",
    "section": "Repeat after me",
    "text": "Repeat after me\nSometimes you need to repeat a task multiple times. Sometimes hundreds. Maybe you need to loop through 1 million pieces of data. Not fun.\nPython’s loops offer us a way to run a section of code multiple times. There are two types: for loops, which run the code once for each element in a sequence (like a list or string), and while loops, which run until some condition is false.\n\nwhile loops\nThese are almost the same as if statements, except for the fact that they run the code multiple times. Let’s begin with a basic conditional\n\nnumber = 5\n\nif number &lt; 10:\n    print(str(number) + \" is less than 10.\")\n\n5 is less than 10.\n\n\n\nUsing str(number) turns the number into a string, which lets us combine it with ” is less than 10.”\n\nThe print statement runs once if the condition is true.\nWhat if we wanted to check all the numbers between 5 and 10? We can use a while loop.\n\nnumber = 5\n\nwhile number &lt; 10:\n    print(str(number) + \" is less than 10.\")\n    number = number + 1\n\n5 is less than 10.\n6 is less than 10.\n7 is less than 10.\n8 is less than 10.\n9 is less than 10.\n\n\nWe’ve done two things\n\nReplace if with while\nIntroduce number = number + 1 to increase the number each time.\n\n\nWithout step 2, we’d have an infinite loop – one that never stops, because the condition would always be true!\n\nWhile loops are useful for repeating code an indeterminate number of times.\n\n\nfor loops\nRealistically, you’re most likely to use a for loop. They’re inherently safer (you can’t have an infinite loop) and often handier.\nIn Python, for loops iterate through a sequence, like the objects in a list. This is more like other languages’ foreach, than most’s for.\nLet’s say you have a list of different fruit\n\nlist_of_fruits = [\"apple\", \"banana\", \"cherry\"]\n\nand you want to run a section of code on \"apple\", then \"banana\", then \"cherry\". Maybe you want to know which ones have the letter “a”. We can start with a for loop\n\nlist_of_fruits = [\"apple\", \"banana\", \"cherry\"]\n\nfor fruit in list_of_fruits:\n    print(fruit)\n\napple\nbanana\ncherry\n\n\nThis loop’s job is to print out the variable fruit. But where is fruit defined? Well, the for loop runs print(fruit) for every element of list_of_fruits, storing the current element in the variable fruit. If we were to write it out explicitly, it would look like\n\nfruit = list_of_fruits[0]\nprint(fruit)\n\nfruit = list_of_fruits[1]\nprint(fruit)\n\nfruit = list_of_fruits[2]\nprint(fruit)\n\napple\nbanana\ncherry\n\n\nLet’s return to our goal: working out which ones have an “a”. We need to put a conditional inside the loop:\n\nlist_of_fruits = [\"apple\", \"banana\", \"cherry\"]\n\nfor fruit in list_of_fruits:\n    if \"a\" in fruit:\n        print(\"a is in \" + fruit)\n    else:\n        print(\"a is not in \" + fruit)\n\na is in apple\na is in banana\na is not in cherry\n\n\n\n\nUsing range\nThere’s a special Python object which is useful for loops, the range. This object ‘contains’ all the numbers between a certain range. For example,\n\nrange(0,5)\n\nrange(0, 5)\n\n\ncover the numbers \\(0-4\\), and is somewhat equivalent to [0,1,2,3,4] (for looping purposes). Of course, we can choose a much bigger number:\n\nfor i in range(0,1000):\n    print(i)\n\nwill print every number between \\(0\\) and \\(1000\\). This can be useful if you need to loop through multiple objects by indexing.",
    "crumbs": [
      "Workshops",
      "Programming Essentials"
    ]
  },
  {
    "objectID": "Workshops/6 - Programming.html#looking-under-the-hood-what-makes-ints-ints",
    "href": "Workshops/6 - Programming.html#looking-under-the-hood-what-makes-ints-ints",
    "title": "Programming Essentials",
    "section": "Looking under the hood: what makes ints ints?",
    "text": "Looking under the hood: what makes ints ints?\nPython is a high level programming language. Its features are often inspired by C and C++, and is itself built in C/C++.\nOne of the major innovations of C++, and object-oriented programming in general, are classes. We won’t go over how to write your own - it’s beyond the scope of this workshop - but they’re worth a conceptual understanding.\nEssentially, all Python variables follow a specific template, known as its class or type. It’s safe to use these interchangebly here. The class is a general template for the variable and it defines what methods (functions) and attributes (variables) live inside the variable.\nInside the variable? Where? Well, a variables contain more than just their value. We use the . operator to access anything besides the value that lives in the variable. For example, all strings have a function called .upper() that makes them uppercase:\n\nrandom_string = \"Hello, this is a sentence.\"\nprint(random_string.upper())\n\nHELLO, THIS IS A SENTENCE.\n\n\nLet me emphasise that all strings have .upper(). That makes upper() a method of strings.\nIn other words, a class is like an empty form that needs filling. The form string has a field called upper() that is filled with the function as defined above.",
    "crumbs": [
      "Workshops",
      "Programming Essentials"
    ]
  },
  {
    "objectID": "Workshops/6 - Programming.html#building-your-own-machines",
    "href": "Workshops/6 - Programming.html#building-your-own-machines",
    "title": "Programming Essentials",
    "section": "Building your own machines",
    "text": "Building your own machines\nWe’ll wrap this session up by looking at custom functions and modules. So far, we’ve only used built-in functions or those from other people’s modules. But we can make our own!\nWe’ve only ever called functions - this is what we do when we use them. All functions need a definition, this is the code that gets run when they’re called.\n\nThe function definition\nFunctions are machines. They take some inputs, run some code with those inputs, and spit out one output. We need to define how they work before we use them. We should specify\n\nA name\nSome inputs\nThe code to run (the machine itself)\nAn output\n\nWe include these in three steps\n\nThe first line of the function definition (the function signature) specifies the name and inputs\nWe then indent all the code we want to run with our inputs\nWe end with a return statement, specifying the output\n\n\ndef insert_function_name_here(input_1_name, input_2_name, ...):\n    # Code code code\n    return output\n\nFor example, let’s create a function that converts centimetres to metres.\n\ndef cm_to_m(value_in_cm):\n    value_in_m = value_in_cm / 100\n    return value_in_m\n\nTaking it apart, we have\n\nName: cm_to_m\nInputs (just one): value_in_cm\nCode (just one line): value_in_m = value_in_cm / 100\nOutput: value_in_m\n\nImportantly, nothing happens when you run this code. Why? Because you’ve only defined the function, you haven’t used it yet.\nTo use this function, we need to call it. Let’s convert \\(10\\text{ cm}\\) to \\(\\text{m}\\).\n\ndef cm_to_m(value_in_cm):\n    value_in_m = value_in_cm / 100\n    return value_in_m\n\ncm_to_m(10)\n\n0.1\n\n\nWhen we call the function, it runs with value_in_cm = 10.\nThat’s it! Every function that you use, built-in or imported, looks like this.\nBecause functions must be defined before called, and defining them produces no output, best practice is to place functions at the top of your script, below the import statements.\n\n\nDocstrings\nSomething you’ll spot on all professional functions are docstrings. This is what Python spits out with the help() function. You can make your own by writing it within triple quotes immediately after the signature ''' ''':\n\ndef cm_to_m(value_in_cm):\n    \"\"\"Converts centimetres to metres\"\"\"\n    value_in_m = value_in_cm / 100\n    return value_in_m\n\ncm_to_m(10)\n\n0.1\n\n\nThat said, the best way to ensure clarity is to use a clear name.\n\n\nCreating modules\nWhat if you need to write lots of functions? We could write unit converters like above for hundreds of possibilities.\nIt’s useful to tuck these away in their own file, so they don’t clog up your main.py.\nLet’s make a new file called conversions.py, and move your function into it. Delete it from your current file.\nThen, to make sure it works, let’s make a new converter too, from centimetres to inches. Your conversions.py file should look like:\n\ndef cm_to_m(value_in_cm):\n    \"\"\"Converts centimetres to metres\"\"\"\n    value_in_m = value_in_cm / 100\n    return value_in_m\n\ndef cm_to_in(value_in_cm):\n    \"\"\"Converts centimetres to inches\"\"\"\n    value_in_inches = value_in_cm / 2.54\n    return value_in_inches\n\nLet’s make another script now called main.py. We’ll run our conversions here by pulling in the functions from conversions.py.\nInside main.py, you’ll need to import conversions.py. To access the functions, you’ll need to use . to look inside the module as usual:\n\nimport conversions\nmetres = conversions.cm_to_m(10)\ninches = conversions.cm_to_in(10)\n\nprint(\"10cm is \" + metres + \"m\")\nprint(\"10cm is \" + inches + \"\\\"\")\n\n\nTo include a double quote ” inside a string made of double quotes, escape it with a backslash: \\\".\n\nCongratulations, you’ve made your first module!",
    "crumbs": [
      "Workshops",
      "Programming Essentials"
    ]
  },
  {
    "objectID": "Projects/datasets.html",
    "href": "Projects/datasets.html",
    "title": "Datasets and tips",
    "section": "",
    "text": "We have nine datasets for you to choose from. We recommend saving your data inside your project.\n\n\n\nDataset\nDescription\nSource\n\n\n\n\nWorld populations\nA summary of world populations and corresponding statistics\nData from a Tidy Tuesday post on 2014 CIA World Factbook data\n\n\nSoccer players\nA summary of approx. 6000 soccer players from 2024\nData from a Kaggle submission.\n\n\nCoffee survey\nA survey of blind coffee tasting results\nData from a Kaggle submission\n\n\nGapminder\nGDP and life expectancy data by country\nData from the Research Bazaar’s R novice tutorial, sourced from Gapminder.\n\n\nMelbourne housing data\nA collection of houses for sale in Melbourne.\nData from a Kaggle submission\n\n\nGoodreads books\nA summary of books on Goodreads.\nData from a Kaggle submission\n\n\nQueensland hospitals\nQueensland emergency department statistics.\nData from the Queensland Government’s Open Data Portal.\n\n\nQueensland fuel prices\nFuel prices by the pump in Queensland\nData from the Queensland Government’s Open Data Portal\n\n\nAeroplane bird strikes\nAeroplane bird strike incidents fron the 90s\nData from a Tidy Tuesday post sourced from an FAA database",
    "crumbs": [
      "Workshops",
      "The Project",
      "Datasets and tips"
    ]
  },
  {
    "objectID": "Projects/datasets.html#datasets",
    "href": "Projects/datasets.html#datasets",
    "title": "Datasets and tips",
    "section": "",
    "text": "We have nine datasets for you to choose from. We recommend saving your data inside your project.\n\n\n\nDataset\nDescription\nSource\n\n\n\n\nWorld populations\nA summary of world populations and corresponding statistics\nData from a Tidy Tuesday post on 2014 CIA World Factbook data\n\n\nSoccer players\nA summary of approx. 6000 soccer players from 2024\nData from a Kaggle submission.\n\n\nCoffee survey\nA survey of blind coffee tasting results\nData from a Kaggle submission\n\n\nGapminder\nGDP and life expectancy data by country\nData from the Research Bazaar’s R novice tutorial, sourced from Gapminder.\n\n\nMelbourne housing data\nA collection of houses for sale in Melbourne.\nData from a Kaggle submission\n\n\nGoodreads books\nA summary of books on Goodreads.\nData from a Kaggle submission\n\n\nQueensland hospitals\nQueensland emergency department statistics.\nData from the Queensland Government’s Open Data Portal.\n\n\nQueensland fuel prices\nFuel prices by the pump in Queensland\nData from the Queensland Government’s Open Data Portal\n\n\nAeroplane bird strikes\nAeroplane bird strike incidents fron the 90s\nData from a Tidy Tuesday post sourced from an FAA database",
    "crumbs": [
      "Workshops",
      "The Project",
      "Datasets and tips"
    ]
  },
  {
    "objectID": "Projects/datasets.html#tips",
    "href": "Projects/datasets.html#tips",
    "title": "Datasets and tips",
    "section": "Tips",
    "text": "Tips\nHere’s a few general tips. In addition, we strongly recommend using popular cheatsheets, which give a quick and easy reference for common packages and functions, and from Data to Viz, which guides you through choosing a visualisation.\n\nHotkeys\n\n\n\n\n\n\n\n\nCode\nHotkey\nDescription\n\n\n\n\n\nF9 (or Fn + F9)\nRun current line\n\n\n# %%\nCtrl + 2\nNew cell (only in Spyder)\n\n\n\nCtrl+Enter\nRun current cell (when in Script)\n\n\n\nCtrl+C\nCancel current operation (when in Console)\n\n\n\n\n\nData manipulation\nUse the pandas package to analyse your data:\nimport pandas as pd\n\nImporting and exporting data\nIn case you’ve forgotten, use the read.csv() function to import data:\ndf = pd.read_csv(\"data/dataset.csv\")\nIf you’d like to export any files from Python to “.csv”, use the .to_csv() method\ndf.to_csv(\"data/output_name.csv\")\n\n\nInitial exploration\nYou’ll want to explore the data to start with - below are a few functions to get started.\n\n\n\n\n\n\n\n\nFunction\nExample\nDescription\n\n\n\n\ndf.columns\n\nReturns the variable names\n\n\ndf.info()\n\nReturns the structure of the dataset (variable names, counts and types)\n\n\ndf[\"variable\"]\n\nReturns a specific column\n\n\npd.unique(\"variable\")\n\nReturns the unique values of a variable\n\n\ndf.describe() or df[\"variable\"].describe()\nReturns a statistical summary of the dataset or a variable\n\n\n\n\n\n\nRemoving nans\nWe can remove nans by filtering with the condition df[\"variable\"].notna():\ndf = df[df[\"variable\"].notna()]\n\n\nTime series data\nIf you’ve picked a dataset with time-series data (e.g. a “date” variable), you should transform that variable so that it visualises better:\ndf[\"variable\"] = pd.to_datetime(df[\"variable\"])\n\n\nCategorical and ordered data\nIf you’re dealing with categorical data, it can be helpful to tell Python\ndf[\"variable\"] = df[\"variable\"].astype(\"category\")\nTo manually specify the order of categories, use the df[\"variable\"].cat.reorder_categories() function and use the ordered = True parameter\ndf[\"variable\"] = df[\"variable\"].cat.reorder_categories([\"cat1\", \"cat2\", ...], ordered = True)\n\nThis is particularly useful for the Coffee survey dataset.\n\nIf you’re dealing with categorical data, look at the pandas guide for inspiration and help.\n\n\nRenaming variables\nSome datasets have cumbersome names for their variables. We can change variable names with df.rename(), sending a dictionary to the columns = parameter:\ndf = df.rename(columns = {\"old_name\": \"new_name\"})\nThis is particularly useful for the World population dataset.\n\nA dictionary is a Python variable with key-value pairs. The structure is key: value, so above we have a dictionary with one key, \"old_name\" and corresponding value \"new_name\". They are created as follows:\nexample_dictionary = {\"key1\": \"value1\",\n                      \"key2\": \"value2\",\n                      \"key3\": \"value3\",\n                      ...}\n\nNote that multiple lines are used purely for readability, you could just as well do this on one line.\n\n\n\n\n\nVisualisation\nYou can make simple visualisations with seaborn’s relplot(), catplot() and displot() functions\nimport seaborn as sns\n\nsns.relplot(data = df, x = \"variable_x\", y = \"variable_y\", hue = \"variable_colour\", ...)\nWe can add plot elements easily with matplotlib.pyplot\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.relplot(data = df, x = \"variable_x\", y = \"variable_y\", hue = \"variable_colour\", ...)\nplt.xlabel(\"x axis label\")\nplt.ylabel(\"y axis label\")",
    "crumbs": [
      "Workshops",
      "The Project",
      "Datasets and tips"
    ]
  }
]